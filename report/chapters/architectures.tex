\chapter{Architectures}

Overall, we developed six different architectures, each one having from two to three different variations depending on the way the \textit{Divine Comedy} was tokenized, for a total of 13 models.
In this chapter we will briefly explain the characteristic of each architecture, but first we must say something about the general aspects that will recur for all of them.

\subsubsection{\textsc{Text Prcoessing}}

First of all, the \texttt{mark} function inside the \texttt{text\_processing.markers} module was developed to insert some special markers in the original \textit{Divine Comedy}.
Together with that, we developed its complementary function \texttt{unmark}, used to get a cleaned text from the ones generated by the Neural Networks.

These markers were intended to give some structure to the text in order to make it easier for the Neural Network to understand the tokenized text. More specifically, the markers are \texttt{"=startofcantica="}, \texttt{"=endofcantica="}, \texttt{"=startofcanto="} \texttt{"=endofcanto="} and \texttt{"tercet"}.

Here is an example of a sample of text before and after the marking:

\begin{paracol}{2}
\scriptsize{\begin{verbatim}
INFERNO

- Canto I

Nel mezzo del cammin di nostra vita
mi ritrovai per una selva oscura,
ché la diritta via era smarrita.

Ahi quanto a dir qual era è cosa dura
esta selva selvaggia e aspra e forte
che nel pensier rinova la paura!
\end{verbatim}}
\switchcolumn
\scriptsize{\begin{verbatim}


=startofcantica=
=startofcanto=
Nel mezzo del cammin di nostra vita
mi ritrovai per una selva oscura,
ché la diritta via era smarrita.
=tercet=
Ahi quanto a dir qual era è cosa dura
esta selva selvaggia e aspra e forte
che nel pensier rinova la paura!
\end{verbatim}}
\end{paracol}


\subsubsection{\textsc{Text Tokenization}}

The tokenization of the text took advantage of Tensorflow Dataset tokenizers, which can be found in the package \texttt{tfds.features.text}.
For our purposes, we built up three of them (a \texttt{char\_tokenizer}, a \texttt{word\_tokenizer} and a \texttt{subword\_tokenizer}), each of which can be found in the package \texttt{text\_processing.tokenizers} of our repository, and is responsible to take care not only of alphanumeric symbols but also of punctuation symbols, newline symbols, white spaces and special markers, all separate tokens that are independently learnt and generated by the Neural Networks.

Each of the six architectures have been tested with both a \textit{Word-Level} and \textit{Subword-Level} tokenization, and for the first architecture we also tried a \textit{Char-Level} implementation.
At the end of the day, the \textit{Subword-Level} tokenization was the one giving best results.
In fact, as the tokenizer was fed with the \textit{Divine Comedy} and it has the ability to split the words into the most frequent subwords, the text could be tokenized in something that vaguely resembled syllables, giving, both from a theoretical and a practical point of view, a great insight to the Neural Network when considering verse structure and rhyming scheme. 

Here is an example of the subword tokenization of a sample of text:

\begin{paracol}{2}
\scriptsize{\begin{verbatim}
    =startofcantica=
    =startofcanto=
    Nel mezzo del cammin di nostra vita
    mi ritrovai per una selva oscura,
    ché la diritta via era smarrita.
    =tercet=
\end{verbatim}}
\switchcolumn
\scriptsize{\begin{verbatim}
    =startofcantica=
    =startofcanto=
    Nel mez zo del cam min di nos tra vit a
    mi rit rov ai per una sel va osc ura ,
    ché la dir itt a via era sma rri ta .
    =tercet=
\end{verbatim}}
\end{paracol}

\subsubsection{\textsc{Hyperparameters and Results}}

All the tested models had their own hyperparameters, which could be related not only to the actual architecture of the Neural Network but also to the construction of the dataset.
In this last case, the hyperparameters were the same for all the models, namely:
\begin{itemize}
    \item the window size to extract the input strings, or rather the sequence length
    \item the number of tokens to skip from one window to the following one, or rather the step length
    \item the train/validation splitting percentage
    \item the size of the batch
    \item the number of training epochs
\end{itemize}

Generally, these parameters can be inferred (e.g. for the sequence length we chose the minimum amount of tokens necessary to get at least the previous three verses entirely) or have a minor influence on the final results, thus, during our experiments to find the best set of hyperparameters, we mainly focused on the hyperparameters of the network.
In any case, we automated the process in order to test the performances of each configuration/model, varying the temperature factor of the generation as well, and stored a summary of the results for each test into a series of \texttt{.txt} files placed in the \texttt{results} folder of the repository.

\subsubsection{\textsc{Optimizer and Loss Function}}

Each model was compiled with either the default or a custom \texttt{Adam} optimizer, where the latter case is referred to the \textit{Transformer} model, whose specifications for a better optimizer were given in the original paper \parencite{vaswani2017attention}.

Instead, as loss function we always used the \texttt{Sparse Categorical Crossentropy} except that for the last model, the \textit{GAN}, that obviously used the \texttt{Binary Crossentropy} of the discriminator to perform backpropagation both during the discriminator and the adversarial training.

\input{chapters/architectures/plain-rnn}

\input{chapters/architectures/full-rnn}

\input{chapters/architectures/seq2seq-rnn}

\input{chapters/architectures/plain-transformer}

\input{chapters/architectures/verse-transformer}

\input{chapters/architectures/transformer-gan}