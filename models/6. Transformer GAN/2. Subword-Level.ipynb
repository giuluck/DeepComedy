{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2. Subword-Level.ipynb","provenance":[{"file_id":"1HNnsZm1rXwOYeYG9e2cZZzMNcDPkoIeN","timestamp":1597162543224},{"file_id":"1bmIPg4r35mXZxxuK68Wj4NhzjWsHawUw","timestamp":1591279397164},{"file_id":"1-r-ETMpyG0nvvJ04rkVvvaLHoNq_lDAi","timestamp":1590830537926},{"file_id":"1I9uENHPUycWU26cmx227U53f6CCfWg2_","timestamp":1590591144637},{"file_id":"1BU8novDKFc9MSPSdfpTrCAB7E01PIbi7","timestamp":1590589899818},{"file_id":"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/text_generation.ipynb","timestamp":1590570606159}],"private_outputs":true,"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"n8bg-vup0iWu","colab_type":"text"},"source":["## **0. Preliminary Settings**"]},{"cell_type":"markdown","metadata":{"id":"Pwe04av4TrA2","colab_type":"text"},"source":["Guarantee access to *Google Drive* to store partial results and checkpoints"]},{"cell_type":"code","metadata":{"id":"i_ybvZZtTpbN","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","checkpoint_path = '/content/drive/My Drive/Colab Notebooks/Models/'\n","results_path = '/content/drive/My Drive/Colab Notebooks/Results/'\n","model_name = '6. Transformer GAN/2. Subword-Level/'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DjtD5j6P5TzW","colab_type":"text"},"source":["First of all, we need to clone the repository to get access to the code and use utility functions inside the notebook"]},{"cell_type":"code","metadata":{"id":"In6Aynn15SAK","colab_type":"code","colab":{}},"source":["!git clone https://github.com/mazzio97/DeepComedy.git\n","\n","project_path = 'DeepComedy/'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SNTAI4EH1UvX","colab_type":"text"},"source":["This folder is then added to the system path so that the modules can be used inside the notebook"]},{"cell_type":"code","metadata":{"id":"qs_mxpvExrY7","colab_type":"code","colab":{}},"source":["import sys\n","\n","sys.path.append(project_path + 'src')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0hne6FCq1ZKZ","colab_type":"text"},"source":["Finally, the *Divine Comedy* is loaded and stored in a variable"]},{"cell_type":"code","metadata":{"id":"EOofZ_881Z2X","colab_type":"code","colab":{}},"source":["with open(project_path + 'res/divine_comedy.txt', 'r', encoding='ISO-8859-1') as f:\n","  divine_comedy = f.read()\n","\n","print(divine_comedy[:231])\n","print('\\n\\n[...]\\n\\n')\n","print(divine_comedy[-266:])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RIV2vg39xPYb","colab_type":"text"},"source":["Also, we set Python's, Numpy's/Keras' and Tensorflow's seeds to guarantee the maximal level of reproducibility\n","\n","> Though, the results could still differ a little bit due to other randomized routines called during the execution and the inner stochasticity introduced by parallel computing"]},{"cell_type":"code","metadata":{"id":"0kGuzr3KxQLY","colab_type":"code","colab":{}},"source":["import random\n","import numpy as np\n","import tensorflow as tf\n","\n","random.seed(0)\n","np.random.seed(0)\n","tf.random.set_seed(0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QsjTDW5425qP","colab_type":"text"},"source":["## **1. Data Processing**"]},{"cell_type":"markdown","metadata":{"id":"IsGmnOvt4FDs","colab_type":"text"},"source":["### ***1.1 Text Mark***\n","\n","We use the provided function `mark` to map the original *Divine Comedy* into a marked version containing:\n","\n","* a marker both at the beginning and at the end of each *cantica*\n","\n","* a marker both at the beginning and at the end of each *canto*\n","\n","* a marker between each couple of *tercets*"]},{"cell_type":"code","metadata":{"id":"-p6yoy-N17hj","colab_type":"code","colab":{}},"source":["from text_processing.markers import mark\n","\n","divine_comedy_marked = mark(divine_comedy)\n","print(divine_comedy_marked[:260])\n","print('\\n\\n[...]\\n\\n')\n","print(divine_comedy_marked[-319:])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_LYDG8rO5JdW","colab_type":"text"},"source":["### ***1.2 The Tokenizer***\n","\n","We use the provided `subword_tokenizer` to tokenize the text into subwords, including punctuation\n","\n","> Some special tokens are reserved to the markers"]},{"cell_type":"code","metadata":{"id":"bE2RTrqY36ZE","colab_type":"code","colab":{}},"source":["from text_processing.tokenizers import subword_tokenizer\n","\n","tokenizer = subword_tokenizer(divine_comedy, target_vocab_size=2048, max_subword_length=3)\n","print(tokenizer.vocab_size, 'tokens:')\n","print()\n","for i, token in enumerate(tokenizer.subwords[:40]):\n","  print(\"'{}'\".format('\\\\n' if token == '\\n' else token))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dM-Dk-OP-bZi","colab_type":"code","colab":{}},"source":["divine_comedy_tokenized = tokenizer.encode(divine_comedy_marked)\n","print(len(divine_comedy_tokenized))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tc-wXffP84Y2","colab_type":"code","colab":{}},"source":["tokenized_sample = divine_comedy_tokenized[:57]\n","print(tokenizer.decode(tokenized_sample))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iY8_AO929kCf","colab_type":"code","colab":{}},"source":["for token in tokenized_sample:\n","  print(token, '-->', tokenizer.decode([token]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TJEMLEjQ8sia","colab_type":"text"},"source":["### ***1.3 Building the Dataset***"]},{"cell_type":"markdown","metadata":{"id":"JN7gx3WVFqMn","colab_type":"text"},"source":["In order to understand which one should be the minimal length of a window sequence so that the net could be able to clearly have an insight about the thyming scheme, we compute which one is the maximal length of an encoded verse and take the minimal length as at least four verses "]},{"cell_type":"code","metadata":{"id":"HXpDR5zf8w16","colab_type":"code","colab":{}},"source":["# the newline token\n","newline = tokenizer.encode('\\n')[0]\n","\n","# the indices of each newline\n","indices = [i + 1 for i, t in enumerate(divine_comedy_tokenized) if t == newline]\n","\n","# the length of each verse (or marker)\n","verses_lengths = [end - start for start, end in zip([0] + indices, indices +  [len(divine_comedy_tokenized)])]\n","\n","# five verses (4 + tercet mark) should be enough to understand the rhyming scheme\n","sequences_lengths = [sum(verses_lengths[i:i+5]) for i in range(len(verses_lengths)-4)]\n","max(sequences_lengths)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WZZCK05QGHOL","colab_type":"text"},"source":["Given that the ***sequence length*** should be at least *109*, we set it as *126 (+2 special start/end tokens = 128)*, then we choose a ***step_length***, namely the value that indicates how often we decide to take a sample and, finally, a ***train/validation split*** percentage\n","\n","> Being the text very dense we cannot take a too small `step_length`, as it will lead both to a prohibitive training time and a lot of overfitting\n","\n","> In order to avoid this behaviour but having the most possible trustworthy set of data, we choose a medium `step_length` together with a small `train_val_split`, so that (at the cost of a quite more expensive training) we could easily monitor overfitting while still using a lot of training data"]},{"cell_type":"code","metadata":{"id":"p4M_wr57FK5j","colab_type":"code","colab":{}},"source":["seq_length = 126\n","step_length = 8\n","batch_size = 128\n","\n","tot_samples = int((len(divine_comedy_tokenized) - seq_length) / step_length)\n","print('Tot Samples:', tot_samples)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H8g_f4WqJ-Sq","colab_type":"text"},"source":["Finally, the tokenized dataset is split into windows of length `seq_length` (*+1*) sampled every `step_length` tokens and these windows are then shared into an *input sequence* and a *target sequence*, both of length `seq_length`, having an offset of one single token"]},{"cell_type":"code","metadata":{"id":"ePAQw8Pk75zq","colab_type":"code","colab":{}},"source":["from tensorflow.data import Dataset\n","\n","def split_input_target(chunk):\n","  input_sequence = tf.concat(([tokenizer.vocab_size], chunk[:-1], [tokenizer.vocab_size+1]), axis=-1)\n","  target_token = tf.one_hot(chunk[-1], tokenizer.vocab_size + 2, dtype=tf.int64)\n","  return input_sequence, target_token\n","\n","dataset = Dataset.from_tensor_slices(divine_comedy_tokenized)\n","dataset = dataset.window(seq_length + 1, step_length, drop_remainder=True)\n","dataset = dataset.flat_map(lambda window: window.batch(seq_length + 1))\n","dataset = dataset.map(split_input_target).shuffle(tot_samples, seed=0)\n","dataset = dataset.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-NysaUmrcbFD","colab_type":"code","colab":{}},"source":["for input, target in dataset.take(1):\n","  print(f'INPUT {input.shape}\\n')\n","  print(tokenizer.decode([t for t in input[0].numpy() if 0 < t < tokenizer.vocab_size]))\n","  print('\\n\\n---------------------\\n\\n')\n","  print(f'TARGET {target.shape}\\n')\n","  print(tokenizer.decode([tf.argmax(target[0])]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QLSm_SHYLPAd","colab_type":"text"},"source":["## **2. Model**"]},{"cell_type":"markdown","metadata":{"id":"1HQ5q_sIXMwh","colab_type":"text"},"source":["### ***2.1 Architecture***\n","\n","The ***Transformer*** is a state-of-the-art model for *Natural Language Processing* and *Machine Translation* tasks proposed by *Vaswani et al.* in 2017 (https://arxiv.org/pdf/1706.03762v5.pdf)\n","\n","* It consists of an *Encoder* and a *Decoder*, each of them made up of a given number of layers having two sub-modules: a *Multi-Head Attention* (with a parametric number of heads) sub-module and a classical *Feed-Foward* sub-module\n","\n","* Also, as it does not use recurrent layers to process strictly sequential data, it both process the input data with a standard token encoding and a *positional encoding* as well\n","\n","> The variable parameters of the model are:\n","> * the number of layers for the *Encoder* and the *Decoder*\n","> * the number of heads for the *Multi-Head Attention* sub-module\n","> * the dimension of all sub-layers in the model, as well as the embedding layers, known as *d_model*\n","> * the inner feed-forward dimension, known as *dff*\n","> * the dropout rate"]},{"cell_type":"markdown","metadata":{"id":"A73pbNHlU9Xg","colab_type":"text"},"source":["#### *2.1.1 Discriminator*"]},{"cell_type":"code","metadata":{"id":"bwQTg-JZU91F","colab_type":"code","colab":{}},"source":["from tensorflow.keras import Input, Model\n","from tensorflow.keras.layers import Embedding, GRU, Reshape, Attention, AdditiveAttention, Concatenate, Dense\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.utils import plot_model\n","\n","embedding_dim = 256\n","gru_units = 1024\n","attention = 'ADD'\n","discriminator_dropout = 0.2\n","\n","input = Input((seq_length + 2,), name='input')\n","input_embedding = Embedding(tokenizer.vocab_size + 2, embedding_dim, name=f'input_embedding')(input)\n","gru, state = GRU(\n","    gru_units, return_sequences=True, return_state=True, stateful=False,\n","    dropout=discriminator_dropout, recurrent_initializer='glorot_uniform', name=f'gru'\n",")(input_embedding)\n","expanded_state = Reshape((1, gru_units), name=f'expanded_state')(state)\n","if attention == 'ADD':\n","  input_features = AdditiveAttention(name=f'input_features')([expanded_state, gru])\n","elif attention == 'MUL':\n","  input_features = Attention(name=f'input_features')([expanded_state, gru])\n","input_features = Reshape((gru_units,), name=f'input_flatten')(input_features)\n","\n","target = Input((tokenizer.vocab_size + 2,), name='target')\n","concatenate = Concatenate(name='concatenate')([input_features, target])\n","output = Dense(1, activation='sigmoid', name='output')(concatenate)\n","\n","discriminator = Model([input, target], output, name='Discriminator')\n","discriminator_optimizer = Adam()\n","\n","display(plot_model(discriminator, show_shapes=True, show_layer_names=False))\n","print()\n","discriminator.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C5vemfZSU_YP","colab_type":"text"},"source":["#### *2.1.2 Generator*"]},{"cell_type":"code","metadata":{"id":"q7c4q8OEVBAH","colab_type":"code","colab":{}},"source":["from transformer.model import *\n","from transformer.functions import *\n","\n","@tf.function\n","def build_step(inp, tar):\n","  tar_inp = tar[:, :-1]\n","  tar_real = tar[:, 1:]  \n","  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n","  predictions, _ = generator(inp, tar_inp, True, enc_padding_mask, combined_mask, dec_padding_mask)\n","\n","num_layers = 3\n","num_heads = 4\n","d_model = 256\n","dff = 512\n","generator_dropout = 0.2\n","input_vocab_size = tokenizer.vocab_size + 2\n","target_vocab_size = tokenizer.vocab_size + 2\n","generator_optimizer = Adam(CustomSchedule(d_model), beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n","\n","generator = Transformer(\n","    num_layers, d_model, num_heads, dff,\n","    input_vocab_size, target_vocab_size,\n","    pe_input=input_vocab_size, pe_target=target_vocab_size,\n","    rate=generator_dropout\n",")\n","\n","for (inp, tar) in dataset.take(1):\n","  build_step(inp, tf.expand_dims(tf.argmax(tar, axis=-1), 1))\n","\n","generator.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sG39RjwxY-xB","colab_type":"text"},"source":["### ***2.2 Training***\n","\n","We need to write a custom training loop as, during the decoding phase, we will need to add one token at a time as well as setting the correct input state, then we will proceed with the training phase, storing every `epochs_interval` epochs the weights of the model in a file that indicates the values of its parameters"]},{"cell_type":"code","metadata":{"id":"FD8eJCgOVKJ9","colab_type":"code","colab":{}},"source":["from tensorflow.keras.losses import binary_crossentropy\n","from tensorflow.keras.metrics import binary_accuracy\n","\n","history = {'discr loss': [], 'discr acc': [], 'adver loss': [], 'adver acc': []}\n","\n","# the discriminator's labels are 1s for real samples and 0s for generated ones\n","# as we are trying to improve the discriminator weights only\n","@tf.function\n","def discriminator_step(inputs, targets):\n","  with tf.GradientTape() as tape:\n","    dec_inputs = tf.expand_dims(tf.ones(batch_size, dtype=tf.int64) * tokenizer.vocab_size, 1)\n","    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inputs, dec_inputs)\n","    logits, _ = generator(inputs, dec_inputs, False, enc_padding_mask, combined_mask, dec_padding_mask)\n","    generated = tf.squeeze(logits)\n","\n","    real_preds = tf.squeeze(discriminator([inputs, targets]))\n","    fake_preds = tf.squeeze(discriminator([inputs, generated]))\n","\n","    loss = binary_crossentropy(tf.ones(batch_size), real_preds) + binary_crossentropy(tf.zeros(batch_size), fake_preds)\n","    acc = binary_accuracy(tf.ones(batch_size), real_preds) + binary_accuracy(tf.zeros(batch_size), fake_preds)\n","\n","  gradients = tape.gradient(loss, discriminator.trainable_variables)\n","  discriminator_optimizer.apply_gradients(zip(gradients, discriminator.trainable_variables))\n","  return loss / 2, acc / 2\n","\n","# the discriminator's labels are 1s as we are trying to improve the generator weights\n","@tf.function\n","def adversarial_step(inputs):\n","  with tf.GradientTape() as tape:\n","    dec_inputs = tf.expand_dims(tf.ones(batch_size, dtype=tf.int64) * tokenizer.vocab_size, 1)\n","    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inputs, dec_inputs)\n","    logits, _ = generator(inputs, dec_inputs, False, enc_padding_mask, combined_mask, dec_padding_mask)\n","    generated = tf.squeeze(logits)\n","\n","    preds = tf.squeeze(discriminator([inputs, generated]))\n","    loss = binary_crossentropy(tf.ones(batch_size), preds)\n","    acc = binary_accuracy(tf.ones(batch_size), preds)\n","\n","  gradients = tape.gradient(loss, generator.trainable_variables)\n","  generator_optimizer.apply_gradients(zip(gradients, generator.trainable_variables))\n","  return loss, acc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uyVQkUkHMXPI","colab_type":"code","colab":{}},"source":["import time\n","from utils.checkpoint import restore_checkpoint, save_checkpoint\n","\n","discriminator_checkpoint_signature = \\\n","  'seq_{} stp_{} btc_{} emb_{} unt_{} att_{} ddr_{} nlr_{} nhd_{} dmd_{} dff_{} gdr_{} epc_'.format(\n","    seq_length, step_length, batch_size,\n","    embedding_dim, gru_units, attention, discriminator_dropout,\n","    num_layers, num_heads, d_model, dff, generator_dropout\n","  )\n","generator_checkpoint_signature = \\\n","  'seq_{} stp_{} btc_{} emb_{} unt_{} att_{} ddr_{} nlr_{} nhd_{} dmd_{} dff_{} gdr_{} epc_'.format(\n","    seq_length, step_length, batch_size,\n","    embedding_dim, gru_units, attention, discriminator_dropout,\n","    num_layers, num_heads, d_model, dff, generator_dropout\n","  )\n","checkpoint_directory = checkpoint_path + model_name\n","initial_epoch = restore_checkpoint(discriminator, checkpoint_directory, discriminator_checkpoint_signature)\n","assert initial_epoch == restore_checkpoint(generator, checkpoint_directory, generator_checkpoint_signature, verbose=False)\n","\n","epochs = 50\n","epochs_interval = 10\n","batches_interval = 20\n","\n","for epoch in range(initial_epoch, epochs):\n","  start = time.time()\n","  history['discr loss'].append(0)\n","  history['discr acc'].append(0)\n","  history['adver loss'].append(0)\n","  history['adver acc'].append(0)\n","  print(f'Starting Epoch {epoch+1}/{epochs}')\n","  \n","  for (batch, (inp, tar)) in enumerate(dataset):\n","    discr_loss, discr_acc = discriminator_step(inp, tar)\n","    adver_loss, adver_acc = adversarial_step(inp)\n","    history['discr loss'][-1] += discr_loss\n","    history['discr acc'][-1] += discr_acc\n","    history['adver loss'][-1] += adver_loss\n","    history['adver acc'][-1] += adver_acc\n","    if (batch + 1) % batches_interval == 0:\n","      print(f'  > Batch {batch+1}', end=' \\t\\t ')\n","      print(f'- discr_loss: {discr_loss:.4f} - discr_acc: {discr_acc:.4f}', end=' ')\n","      print(f'- adver_loss: {adver_loss:.4f} - adver_acc: {adver_acc:.4f}')\n","\n","  history['discr loss'][-1] /= batch\n","  history['discr acc'][-1] /= batch\n","  history['adver loss'][-1] /= batch\n","  history['adver acc'][-1] /= batch\n","  elapsed = time.time() - start\n","  print(f'Ending Epoch {epoch+1}/{epochs}', end=' \\t ')\n","  print(f'- discr_loss: {history[\"discr loss\"][-1]:.4f} - discr_acc: {history[\"discr acc\"][-1]:.4f}', end=' ')\n","  print(f'- adver_loss: {history[\"adver loss\"][-1]:.4f} - adver_acc: {history[\"adver acc\"][-1]:.4f}')\n","  print(f'Elapsed Time {elapsed:.2f}s\\n')\n","\n","  save_checkpoint(discriminator, epoch, checkpoint_directory, discriminator_checkpoint_signature, epochs_interval)\n","  save_checkpoint(generator, epoch, checkpoint_directory, generator_checkpoint_signature, epochs_interval, verbose=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nuvvp5hygYOv","colab_type":"text"},"source":["Here's a graphical representation of the improvement of the model, with respect both to the loss and the accuracy, across the epochs"]},{"cell_type":"code","metadata":{"id":"6GPN1Jo0gXpQ","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","\n","if epochs - initial_epoch > 0:\n","  sns.set_style('darkgrid')\n","  sns.set_context('notebook')\n","  plt.figure(figsize=(12, 5))\n","\n","  x = np.arange(initial_epoch, epochs) + 1\n","\n","  plt.subplot(1, 2, 1)\n","  plt.plot(x, history['discr loss'], label='discriminator')\n","  plt.plot(x, history['adver loss'], label='adversarial')\n","  plt.legend()\n","  plt.title('Loss')\n","\n","  plt.subplot(1, 2, 2)\n","  plt.plot(x, history['discr acc'], label='discriminator')\n","  plt.plot(x, history['adver acc'], label='adversarial')\n","  plt.legend()\n","  plt.title('Accuracy')\n","\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TGKbUSPZZOw_","colab_type":"text"},"source":["## **3. Generation**\n","\n","The generation is based on the trained model and it uses a `temperature_factor` to allow some degree of randomness\n","\n","> The next token is chosen among a subset of those having a probability which is at least `1 / temperature_factor` with respect to the maximal one\n","\n","> It goes without saying that a higher `temperature_factor` leads to a more explorative generation, while a lower `temperature_factor` leads to a more conservative one (in particular, with `temperature_factor = 1` the generation is completely deterministic)"]},{"cell_type":"code","metadata":{"id":"z2zd-C3Hle31","colab_type":"code","colab":{}},"source":["from tensorflow.nn import softmax\n","from text_processing.markers import unmark, MARKERS\n","\n","def evaluate(inp_sentence):\n","  # the encoder input is the input sentence surrounded by a start and an end token\n","  encoder_input = tf.expand_dims([tokenizer.vocab_size] + inp_sentence + [tokenizer.vocab_size + 1], 0)\n","\n","  # the decoder input is the same sentence, without the first character, preceded by a start token\n","  decoder_input = tf.expand_dims([tokenizer.vocab_size] + inp_sentence[1:], 0)  \n","\n","  # we now call the generator to get the logits tensor\n","  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, decoder_input)\n","  logits, attention_weights = generator(\n","      encoder_input, decoder_input, False,\n","      enc_padding_mask, combined_mask, dec_padding_mask\n","  )\n","\n","  # finally, we return the logits for the decoded character\n","  return logits[0, len(inp_sentence)-1, :].numpy()[:tokenizer.vocab_size]\n","\n","def generate(\n","    input_string=divine_comedy_marked[:386], # first three tercets of the comedy\n","    max_iterations=4000, end_marker=MARKERS['canto end'],\n","    temperature_factor=1.0, verbose=False\n","):\n","  # at the beginning, the generated string is the encoding of the input string\n","  generated_string = tokenizer.encode(input_string)\n","\n","  for i in range(max_iterations):\n","    # the input sequence is made up of the last 'seq_length' tokens of the generated string\n","    input_sequence = generated_string[-seq_length:]\n","\n","    # as the evaluate function returns logits, we need to apply a softmax activation to get probabilities\n","    probabilities = softmax(evaluate(input_sequence)).numpy()\n","\n","    # we take a subset of possible tokens whose probability is at least 1/temperature_factor of the maximal one\n","    indices = np.arange(tokenizer.vocab_size)[probabilities >= probabilities.max() / temperature_factor]\n","\n","    # we renormalize this subset using, again, a softmax activation\n","    probabilities = softmax(probabilities[probabilities >= probabilities.max() / temperature_factor]).numpy()\n","\n","    # the id is randomly chosen among the indices according to the computed probabilities\n","    predicted_id = np.random.choice(indices, size=1, p=probabilities)[0]\n","\n","    # the id is then mappend into a token from the vocabulary\n","    predicted_token = tokenizer.decode([predicted_id])\n","    if verbose:\n","      print(predicted_token, end='')\n","\n","    # if the token coincides with the end marker, the generation is interrupted, otherwise the token is appended \n","    if predicted_token == end_marker:\n","      break\n","    generated_string.append(predicted_id)\n","  \n","  # we finally return the decoded (and unmarked) string, excluding the input provided by the user\n","  return unmark(tokenizer.decode(generated_string)[len(input_string):])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZDf5e4kzgLY_","colab_type":"text"},"source":["### ***3.2 Results***\n","\n","We then try different kind of temperatures and take an average evaluation, then, finally, we use the `store` utility function to store the results in a *.txt* file"]},{"cell_type":"code","metadata":{"id":"R_93--aGpSLA","colab_type":"code","colab":{}},"source":["from metrics.store import store\n","\n","result_dir = results_path + model_name\n","result_file = 'seq_{} stp_{} btc_{} emb_{} unt_{} att_{} ddr_{} nlr_{} nhd_{} dmd_{} dff_{} gdr_{}'.format(\n","    seq_length, step_length, batch_size,\n","    embedding_dim, gru_units, attention, discriminator_dropout,\n","    num_layers, num_heads, d_model, dff, generator_dropout\n",")\n","\n","print('Generating Temperature 1...')\n","samples = [generate(temperature_factor=1)]\n","store(result_dir, result_file, temperature=1, sample_texts=samples, original_text=divine_comedy)\n","\n","repetitions = 5\n","for temperature in [2, 3, 5, 10]:\n","  samples = []\n","  for r in range(repetitions):\n","    print(f'Generating Temperature {temperature} (repetition {r+1})...')\n","    samples.append(generate(temperature_factor=temperature))\n","  store(result_dir, result_file, temperature, sample_texts=samples, original_text=divine_comedy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DeOhwkDNpXLi","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}