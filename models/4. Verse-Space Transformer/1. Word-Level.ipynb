{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "1. Word-Level.ipynb",
   "provenance": [
    {
     "file_id": "1bmIPg4r35mXZxxuK68Wj4NhzjWsHawUw",
     "timestamp": 1591279397164
    },
    {
     "file_id": "1-r-ETMpyG0nvvJ04rkVvvaLHoNq_lDAi",
     "timestamp": 1590830537926
    },
    {
     "file_id": "1I9uENHPUycWU26cmx227U53f6CCfWg2_",
     "timestamp": 1590591144637
    },
    {
     "file_id": "1BU8novDKFc9MSPSdfpTrCAB7E01PIbi7",
     "timestamp": 1590589899818
    },
    {
     "file_id": "https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/text_generation.ipynb",
     "timestamp": 1590570606159
    }
   ],
   "private_outputs": true,
   "collapsed_sections": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8bg-vup0iWu",
    "colab_type": "text"
   },
   "source": [
    "## **0. Preliminary Settings**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QRX8cEBb1brn",
    "colab_type": "text"
   },
   "source": [
    "Guarantee access to *Google Drive* to store partial results and checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "REof4kXgRY5_",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "checkpoint_path = '/content/drive/My Drive/Colab Notebooks/Models/'\n",
    "results_path = '/content/drive/My Drive/Colab Notebooks/Results/'\n",
    "model_name = '4. Verse-Space Transformer/1. Word-Level/'"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DjtD5j6P5TzW",
    "colab_type": "text"
   },
   "source": [
    "First of all, we need to clone the repository to get access to the code and use utility functions inside the notebook"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "In6Aynn15SAK",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "!git clone https://github.com/mazzio97/DeepComedy.git\n",
    "\n",
    "project_path = 'DeepComedy/'"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SNTAI4EH1UvX",
    "colab_type": "text"
   },
   "source": [
    "This folder is then added to the system path so that the modules can be used inside the notebook"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qs_mxpvExrY7",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(project_path + 'src')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hne6FCq1ZKZ",
    "colab_type": "text"
   },
   "source": [
    "Finally, the *Divine Comedy* is loaded and stored in a variable"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EOofZ_881Z2X",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "with open(project_path + 'res/divine_comedy.txt', 'r', encoding='ISO-8859-1') as f:\n",
    "  divine_comedy = f.read()\n",
    "\n",
    "print(divine_comedy[:231])\n",
    "print('\\n\\n[...]\\n\\n')\n",
    "print(divine_comedy[-266:])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RIV2vg39xPYb",
    "colab_type": "text"
   },
   "source": [
    "Also, we set Python's, Numpy's/Keras' and Tensorflow's seeds to guarantee the maximal level of reproducibility\n",
    "\n",
    "> Though, the results could still differ a little bit due to other randomized routines called during the execution and the inner stochasticity introduced by parallel computing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0kGuzr3KxQLY",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QsjTDW5425qP",
    "colab_type": "text"
   },
   "source": [
    "## **1. Data Processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IsGmnOvt4FDs",
    "colab_type": "text"
   },
   "source": [
    "### ***1.1 Text Mark***\n",
    "\n",
    "We use the provided function `mark` to map the original *Divine Comedy* into a marked version containing:\n",
    "\n",
    "* a marker both at the beginning and at the end of each *cantica*\n",
    "\n",
    "* a marker both at the beginning and at the end of each *canto*\n",
    "\n",
    "* a marker between each couple of *tercets*"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-p6yoy-N17hj",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from text_processing.markers import mark\n",
    "\n",
    "divine_comedy_marked = mark(divine_comedy)\n",
    "print(divine_comedy_marked[:260])\n",
    "print('\\n\\n[...]\\n\\n')\n",
    "print(divine_comedy_marked[-319:])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y5yz_cPdAWEO",
    "colab_type": "text"
   },
   "source": [
    "### ***1.2 Extracting the Verses***\n",
    "\n",
    "We want to build a dataset in which the input sequence represents a piece of the *Divine Comedy* going from verse *i* to verse *i+n* and the target sequence represents a piece of the *Divine Comedy* going from verse *i+1* to verse *i+n+1*, thus we need at first to split the dataset and get a list of verses"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QK_reSlvAWXq",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "divine_comedy_split = divine_comedy_marked.split('\\n')\n",
    "\n",
    "for i, verse in enumerate(divine_comedy_split[:20]):\n",
    "  print(f'{i+1:02} --> {verse}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kRn35ESvEWun",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "print(len(divine_comedy_split))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJEMLEjQ8sia",
    "colab_type": "text"
   },
   "source": [
    "### ***1.3 Building the Dataset***\n",
    "\n",
    "As we know what is the rhyming scheme of the *Divine Comedy*, we know that we will need at least the last *3* verses (*3 actual verses or 2 actual verses + 1 marker verse to indicate the end of the tercet*) to predict a correct fifth one, so we set `seq_length = 3`\n",
    "\n",
    "> Differently from single-token models, here we have a lower amount of samples and a greater variability (indeed, the dataset is less dense), thus we can choose a `step_length` of *1* and a larger `train_val_split`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "p4M_wr57FK5j",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "seq_length = 3\n",
    "step_length = 1\n",
    "batch_size = 64\n",
    "train_val_split = 0.7\n",
    "\n",
    "tot_samples = int((len(divine_comedy_split) - seq_length) / step_length)\n",
    "train_samples = round(tot_samples * train_val_split)\n",
    "\n",
    "print('Train Samples:', train_samples)\n",
    "print('  Val Samples:', tot_samples - train_samples)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S6yDuoRZEa0a",
    "colab_type": "text"
   },
   "source": [
    "Now, we map the list of verses into a dataset taking *4* verses per time, and splitting them into an input string of the first *3* verses and a target string of the last *3* verses"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ePAQw8Pk75zq",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from tensorflow.data import Dataset\n",
    "from tensorflow.strings import reduce_join\n",
    "\n",
    "def split_input_target(chunk):\n",
    "  input_text = reduce_join(chunk[:-1], separator='\\n') + '\\n'\n",
    "  target_text = reduce_join(chunk[1:], separator='\\n')\n",
    "  return input_text, target_text\n",
    "\n",
    "dataset = Dataset.from_tensor_slices(divine_comedy_split)\n",
    "dataset = dataset.window(seq_length + 1, step_length, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(seq_length + 1))\n",
    "dataset = dataset.map(split_input_target).shuffle(tot_samples, seed=0)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CgKUP6qNEiKo",
    "colab_type": "text"
   },
   "source": [
    "Finally, we encode each block of the comedy using the provided `word_tokenizer` to tokenize the text into words, including punctuation\n",
    "\n",
    "> Some special tokens are reserved to the markers"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bE2RTrqY36ZE",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from text_processing.tokenizers import word_tokenizer\n",
    "\n",
    "tokenizer = word_tokenizer(divine_comedy)\n",
    "print(tokenizer.vocab_size, 'tokens:')\n",
    "print()\n",
    "for i, token in enumerate(tokenizer.tokens[:40]):\n",
    "  print(\"'{}'\".format('\\\\n' if token == '\\n' else token))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-wqwv8CZKppn",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def encode_dataset(input_dataset, target_dataset):\n",
    "  def encode_sample(input, target):\n",
    "    input = [tokenizer.vocab_size] + tokenizer.encode(input.numpy()) + [tokenizer.vocab_size+1]\n",
    "    target = [tokenizer.vocab_size] + tokenizer.encode(target.numpy()) + [tokenizer.vocab_size+1]\n",
    "    return input, target\n",
    "\n",
    "  input_dataset, target_dataset = tf.py_function(encode_sample, [input_dataset, target_dataset], [tf.int64, tf.int64])\n",
    "  input_dataset.set_shape([None])\n",
    "  target_dataset.set_shape([None])\n",
    "  return input_dataset, target_dataset\n",
    "\n",
    "train_dataset = dataset.take(train_samples).map(encode_dataset)\n",
    "train_dataset = train_dataset.cache()\n",
    "train_dataset = train_dataset.padded_batch(batch_size)\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "val_dataset = dataset.skip(train_samples).map(encode_dataset)\n",
    "val_dataset = val_dataset.cache()\n",
    "val_dataset = val_dataset.padded_batch(batch_size)\n",
    "val_dataset = val_dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3bZAWLNaJr93",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "for input, target in train_dataset.take(1):\n",
    "  input = input.numpy()[0]\n",
    "  target = target.numpy()[0]\n",
    "\n",
    "  print(f'Input  Shape: {input.shape}')\n",
    "  print(f'Target Shape: {target.shape}')\n",
    "  print()\n",
    "\n",
    "  print('INPUT:\\n')\n",
    "  print(tokenizer.decode([token for token in input if token < tokenizer.vocab_size]))\n",
    "  print('\\n\\n---------------------\\n\\n')\n",
    "  print('TARGET:\\n')\n",
    "  print(tokenizer.decode([token for token in target if token < tokenizer.vocab_size]))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLSm_SHYLPAd",
    "colab_type": "text"
   },
   "source": [
    "## **2. Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1HQ5q_sIXMwh",
    "colab_type": "text"
   },
   "source": [
    "### ***2.1 Architecture***\n",
    "\n",
    "The model consists of an initial *Embedding* layer that maps the tokenized characters into a dense vector which is then passed to one or two *RNN* layer(s) and, eventually, to a final *Dense* layer, post-processed using *softmax* activation, which outputs the probability of each token\n",
    "\n",
    "> The variable parameters of the model are:\n",
    "> * the dimension of the *Embedding* layer\n",
    "> * the kind of *RNN* (*GRU* or *LSTM*)\n",
    "> * the number of units of the *RNN* layers\n",
    "> * the forward and recurrent dropout rates "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lPZDGEeATZ7g",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from transformer.encoder import *\n",
    "from transformer.decoder import *\n",
    "from transformer.attention import *\n",
    "from transformer.functions import *\n",
    "from transformer.model import *\n",
    "\n",
    "history = {'train loss': [], 'train acc': [], 'val loss': [], 'val acc': []}\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='val_accuracy')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)\n",
    "\n",
    "step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=step_signature)\n",
    "def train_step(inp, tar):\n",
    "  tar_inp = tar[:, :-1]\n",
    "  tar_real = tar[:, 1:]\n",
    "  \n",
    "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions, _ = model(inp, tar_inp, True, enc_padding_mask, combined_mask, dec_padding_mask)\n",
    "    loss = loss_function(tar_real, predictions)\n",
    "\n",
    "  gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "  \n",
    "  train_loss(loss)\n",
    "  train_accuracy(tar_real, predictions)\n",
    "\n",
    "@tf.function(input_signature=step_signature)\n",
    "def val_step(inp, tar):\n",
    "  tar_inp = tar[:, :-1]\n",
    "  tar_real = tar[:, 1:]\n",
    "  \n",
    "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions, _ = model(inp, tar_inp, True, enc_padding_mask, combined_mask, dec_padding_mask)\n",
    "    loss = loss_function(tar_real, predictions)\n",
    "\n",
    "  val_loss(loss)\n",
    "  val_accuracy(tar_real, predictions)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ViSIDkxHTa_b",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "num_layers = 6\n",
    "num_heads = 8\n",
    "d_model = 128\n",
    "dff = 512\n",
    "dropout = 0.2\n",
    "input_vocab_size = tokenizer.vocab_size + 2\n",
    "target_vocab_size = tokenizer.vocab_size + 2\n",
    "optimizer = Adam(CustomSchedule(d_model), beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "model = Transformer(\n",
    "    num_layers, d_model, num_heads, dff,\n",
    "    input_vocab_size, target_vocab_size,\n",
    "    pe_input=input_vocab_size, pe_target=target_vocab_size,\n",
    "    rate=dropout\n",
    ")\n",
    "\n",
    "for (inp, tar) in train_dataset.take(1):\n",
    "  val_step(inp, tar)\n",
    "\n",
    "model.summary()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sG39RjwxY-xB",
    "colab_type": "text"
   },
   "source": [
    "### ***2.2 Training***\n",
    "\n",
    "We can now proceed with the training phase, storing every `epochs_interval` epochs the weights of the model in a file that indicates the values of its parameters"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uyVQkUkHMXPI",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import time\n",
    "from utils.checkpoint import restore_checkpoint, save_checkpoint\n",
    "\n",
    "checkpoint_signature = 'seq_{} stp_{} btc_{} tvs_{} nlr_{} nhd_{} dmd_{} dff_{} drp_{} epc_'.format(\n",
    "    seq_length, step_length, batch_size, train_val_split,\n",
    "    num_layers, num_heads, d_model, dff, dropout\n",
    ")\n",
    "checkpoint_directory = checkpoint_path + model_name\n",
    "initial_epoch = restore_checkpoint(model, checkpoint_directory, checkpoint_signature)\n",
    "\n",
    "epochs = 50 + initial_epoch\n",
    "epochs_interval = 10\n",
    "batches_interval = 20\n",
    "\n",
    "for epoch in range(initial_epoch, epochs):\n",
    "  start = time.time()\n",
    "  train_loss.reset_states()\n",
    "  train_accuracy.reset_states()\n",
    "  val_loss.reset_states()\n",
    "  val_accuracy.reset_states()\n",
    "  print(f'Starting Epoch {epoch+1}/{epochs}')\n",
    "  \n",
    "  for (batch, (inp, tar)) in enumerate(train_dataset):\n",
    "    train_step(inp, tar)\n",
    "    if (batch + 1) % batches_interval == 0:\n",
    "      print(f'  > Batch {batch+1}', end=' \\t\\t ')\n",
    "      print(f'- train_loss: {train_loss.result():.4f} - train_acc: {train_accuracy.result():.4f}')      \n",
    "  history['train loss'].append(train_loss.result())\n",
    "  history['train acc'].append(train_accuracy.result())\n",
    "\n",
    "  for (batch, (inp, tar)) in enumerate(val_dataset):\n",
    "    val_step(inp, tar)  \n",
    "  history['val loss'].append(val_loss.result())\n",
    "  history['val acc'].append(val_accuracy.result())\n",
    "\n",
    "  elapsed = time.time() - start\n",
    "  print(f'Ending Epoch {epoch+1}/{epochs}', end=' \\t ')\n",
    "  print(f'- train_loss: {history[\"train loss\"][-1]:.4f} - train_acc: {history[\"train acc\"][-1]:.4f}', end=' ')\n",
    "  print(f'- val_loss: {history[\"val loss\"][-1]:.4f} - val_acc: {history[\"val acc\"][-1]:.4f}')\n",
    "  print(f'Elapsed Time {elapsed:.2f}s\\n')\n",
    "\n",
    "  save_checkpoint(model, epoch, checkpoint_directory, checkpoint_signature, epochs_interval, verbose=True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nuvvp5hygYOv",
    "colab_type": "text"
   },
   "source": [
    "Here's a graphical representation of the improvement of the model, with respect both to the loss and the accuracy, across the epochs"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6GPN1Jo0gXpQ",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "if epochs - initial_epoch > 0:\n",
    "  sns.set_style('darkgrid')\n",
    "  sns.set_context('notebook')\n",
    "  plt.figure(figsize=(12, 5))\n",
    "\n",
    "  x = np.arange(epochs) + initial_epoch + 1\n",
    "\n",
    "  plt.subplot(1, 2, 1)\n",
    "  plt.plot(x, history['train loss'], label='train')\n",
    "  plt.plot(x, history['val loss'], label='val')\n",
    "  plt.legend()\n",
    "  plt.title('Loss')\n",
    "\n",
    "  plt.subplot(1, 2, 2)\n",
    "  plt.plot(x, history['train acc'], label='train')\n",
    "  plt.plot(x, history['val acc'], label='val')\n",
    "  plt.legend()\n",
    "  plt.title('Accuracy')\n",
    "\n",
    "  plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGKbUSPZZOw_",
    "colab_type": "text"
   },
   "source": [
    "## **3. Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05Mk_Fqnjj2I",
    "colab_type": "text"
   },
   "source": [
    "### ***3.1 Generation***\n",
    "\n",
    "The generation is based on the trained model and it uses a `temperature_factor` to allow some degree of randomness\n",
    "\n",
    "> The next token is chosen among a subset of those having a probability which is at least `1 / temperature_factor` with respect to the maximal one\n",
    "\n",
    "> It goes without saying that a higher `temperature_factor` leads to a more explorative generation, while a lower `temperature_factor` leads to a more conservative one (in particular, with `temperature_factor = 1` the generation is completely deterministic)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zyq4PC2hSL1Z",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from tensorflow.nn import softmax\n",
    "from text_processing.markers import unmark, MARKERS\n",
    "\n",
    "newline_token = tokenizer.encode('\\n')[0]\n",
    "\n",
    "def evaluate(inp_sentence, max_length=30, temperature_factor=1, verbose=False):\n",
    "  # the encoder input is the input sentence surrounded by a start and an end token\n",
    "  encoder_input = tf.expand_dims([tokenizer.vocab_size] + inp_sentence + [tokenizer.vocab_size + 1], 0)\n",
    "\n",
    "  # the decoder input is the same sentence, without the first character, preceded by a start token\n",
    "  decoder_input = tf.expand_dims([tokenizer.vocab_size] + inp_sentence[1:], 0)\n",
    "\n",
    "  # the final output of the evaluation (initially, this is an empty list)\n",
    "  output = []\n",
    "\n",
    "  # we repeat the process to get the entire verse (until the end token or the newline token is predicted)\n",
    "  for i in range(max_length):\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, decoder_input)  \n",
    "    logits, _ = model(\n",
    "        encoder_input, decoder_input, False,\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask\n",
    "    )\n",
    "\n",
    "    # we get the probabilities for the decoded token (i-th token after the last token of the input sentence)\n",
    "    probabilities = softmax(logits[0, len(inp_sentence)+i-1, :tokenizer.vocab_size]).numpy()\n",
    "\n",
    "    # we take a subset of possible tokens whose probability is at least 1/temperature_factor of the maximal one\n",
    "    indices = np.arange(tokenizer.vocab_size)[probabilities >= probabilities.max() / temperature_factor]\n",
    "\n",
    "    # we renormalize this subset using, again, a softmax activation\n",
    "    probabilities = softmax(probabilities[probabilities >= probabilities.max() / temperature_factor]).numpy()\n",
    "    \n",
    "    # the id is randomly chosen among the indices according to the computed probabilities\n",
    "    predicted_id = np.random.choice(indices, size=1, p=probabilities)[0]\n",
    "    \n",
    "    # if the token coincides with the nd token or the newline token, the generation is interrupted\n",
    "    if predicted_id in [newline_token, tokenizer.vocab_size+1]:\n",
    "      break\n",
    "\n",
    "    # otherwise the token is appended both to the new decoder input and to the final output\n",
    "    decoder_input = tf.concat([decoder_input, [[predicted_id]]], axis=-1)\n",
    "    output.append(predicted_id)\n",
    "\n",
    "    if verbose:\n",
    "      print(tokenizer.decode([predicted_id]), end='')\n",
    "\n",
    "  return output\n",
    "\n",
    "def generate(\n",
    "    input_string=divine_comedy_marked[:386], # first three tercets of the comedy\n",
    "    max_iterations=250, end_marker=MARKERS['canto end'],\n",
    "    temperature_factor=1.0, verbose=False\n",
    "):\n",
    "  # at the beginning, the generated string is the encoding of the input string (plus a newline character)\n",
    "  generated_string = input_string + '\\n'\n",
    "\n",
    "  for i in range(max_iterations):\n",
    "    # the input sequence is made up of the last 'seq_length' verses of the generated string\n",
    "    input_sequence = input_string.split('\\n')[-seq_length:]\n",
    "    input_sequence = tokenizer.encode('\\n'.join(input_sequence))\n",
    "\n",
    "    # the generated verse is then decoded\n",
    "    generated_verse = tokenizer.decode(evaluate(input_sequence))\n",
    "    if verbose:\n",
    "      print()\n",
    "\n",
    "    # if the verse coincides with the end marker, the generation is interrupted, otherwise it is appended with a newline\n",
    "    if generated_verse == end_marker:\n",
    "      break\n",
    "    generated_string += generated_verse + '\\n'\n",
    "  \n",
    "  # we finally return the decoded (and unmarked) string, excluding the input provided by the user\n",
    "  return unmark(generated_string[len(input_string):])\n",
    "\n",
    "generate(verbose=True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dgXV_DFtpYDl",
    "colab_type": "text"
   },
   "source": [
    "### ***3.2 Results***\n",
    "\n",
    "We then try different kind of temperatures and take an average evaluation, then, finally, we use the `store` utility function to store the results in a *.txt* file"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "R_93--aGpSLA",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import os\n",
    "from metrics.store import store\n",
    "\n",
    "result_dir = results_path + model_name\n",
    "result_file = 'seq_{} stp_{} btc_{} tvs_{} nlr_{} nhd_{} dmd_{} dff_{} drp_{}.txt'.format(\n",
    "    seq_length, step_length, batch_size, train_val_split,\n",
    "    num_layers, num_heads, d_model, dff, dropout\n",
    ")\n",
    "\n",
    "print('Generating Temperature 1...')\n",
    "samples = [generate(temperature_factor=1)]\n",
    "store(result_dir, result_file, temperature=1, sample_texts=samples, original_text=divine_comedy)\n",
    "\n",
    "repetitions = 5\n",
    "for temperature in [2, 3, 5, 10, 20, 50, 100]:\n",
    "  samples = []\n",
    "  for r in range(repetitions):\n",
    "    print(f'Generating Temperature {temperature} (repetition {r+1})...')\n",
    "    samples.append(generate(temperature_factor=temperature))\n",
    "  store(result_dir, result_file, temperature, sample_texts=samples, original_text=divine_comedy)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qk15dWMUyklE",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}