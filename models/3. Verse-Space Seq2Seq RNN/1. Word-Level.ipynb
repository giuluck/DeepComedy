{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1. Word-Level.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8bg-vup0iWu",
        "colab_type": "text"
      },
      "source": [
        "## **0. Preliminary Settings**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjtD5j6P5TzW",
        "colab_type": "text"
      },
      "source": [
        "First of all, we need to clone the repository to get access to the code and use utility functions inside the notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "In6Aynn15SAK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/mazzio97/DeepComedy.git\n",
        "\n",
        "project_path = 'DeepComedy/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNTAI4EH1UvX",
        "colab_type": "text"
      },
      "source": [
        "This folder is then added to the system path so that the modules can be used inside the notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qs_mxpvExrY7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "\n",
        "sys.path.append(project_path + 'src')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hne6FCq1ZKZ",
        "colab_type": "text"
      },
      "source": [
        "Finally, the *Divine Comedy* is loaded and stored in a variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOofZ_881Z2X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(project_path + 'res/divine_comedy.txt', 'r', encoding='ISO-8859-1') as f:\n",
        "  divine_comedy = f.read()\n",
        "\n",
        "print(divine_comedy[:231])\n",
        "print('\\n\\n[...]\\n\\n')\n",
        "print(divine_comedy[-266:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIV2vg39xPYb",
        "colab_type": "text"
      },
      "source": [
        "Also, we set seeds for Python, Numpy/Keras and Tensorflow to guarantee the maximal level of reproducibility\n",
        "\n",
        "> Though, the results could still differ a little bit due to other randomized routines called during the execution and the inner stochasticity introduced by parallel computing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kGuzr3KxQLY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "tf.random.set_seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsjTDW5425qP",
        "colab_type": "text"
      },
      "source": [
        "## **1. Data Processing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsGmnOvt4FDs",
        "colab_type": "text"
      },
      "source": [
        "### ***1.1 Text Mark***\n",
        "\n",
        "We use the provided function `mark` to map the original *Divine Comedy* into a marked version containing:\n",
        "\n",
        "* a marker both at the beginning and at the end of each *cantica*\n",
        "\n",
        "* a marker both at the beginning and at the end of each *canto*\n",
        "\n",
        "* a marker between each couple of *tercets*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-p6yoy-N17hj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from text_processing.markers import mark\n",
        "\n",
        "divine_comedy_marked = mark(divine_comedy)\n",
        "print(divine_comedy_marked[:260])\n",
        "print('\\n\\n[...]\\n\\n')\n",
        "print(divine_comedy_marked[-319:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5yz_cPdAWEO",
        "colab_type": "text"
      },
      "source": [
        "### ***1.2 Extracting the Verses***\n",
        "\n",
        "We want to build a dataset in which the input sequence represents a piece of the *Divine Comedy* going from verse *i* to verse *i+n* and the target sequence represents a piece of the *Divine Comedy* going from verse *i* to verse *i+n+1*, thus we need at first to split the dataset and get a list of verses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QK_reSlvAWXq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "divine_comedy_split = divine_comedy_marked.split('\\n')\n",
        "\n",
        "for i, verse in enumerate(divine_comedy_split[:20]):\n",
        "  print(f'{i+1:02} --> {verse}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRn35ESvEWun",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(divine_comedy_split))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJEMLEjQ8sia",
        "colab_type": "text"
      },
      "source": [
        "### ***1.3 Building the Dataset***\n",
        "\n",
        "As we know what is the rhyming scheme of the *Divine Comedy*, we know that we will need at least the last *3* verses (*3 actual verses or 2 actual verses + 1 marker verse to indicate the end of the tercet*) to predict a correct fifth one, so we set `seq_length = 3`\n",
        "\n",
        "> Differently from single-token models, here we have a lower amount of samples and a greater variability (indeed, the dataset is less dense), thus we can choose a `step_length` of *1* and a larger `train_val_split`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4M_wr57FK5j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq_length = 3\n",
        "step_length = 1\n",
        "batch_size = 64\n",
        "train_val_split = 0.7\n",
        "\n",
        "tot_samples = int((len(divine_comedy_split) - seq_length) / step_length)\n",
        "train_samples = round(tot_samples * train_val_split)\n",
        "\n",
        "print('Train Samples:', train_samples)\n",
        "print('  Val Samples:', tot_samples - train_samples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6yDuoRZEa0a",
        "colab_type": "text"
      },
      "source": [
        "Now, we map the list of verses into a dataset taking *4* verses per time, and splitting them into an input string of the first *3* verses and a target string containing all the *4* verses taken into consideration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePAQw8Pk75zq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.data import Dataset\n",
        "from tensorflow.strings import reduce_join\n",
        "\n",
        "def split_input_target(chunk):\n",
        "  input_text = reduce_join(chunk[:-1], separator='\\n') + '\\n'\n",
        "  target_text = reduce_join(chunk, separator='\\n') + '\\n'\n",
        "  return input_text, target_text\n",
        "\n",
        "dataset = Dataset.from_tensor_slices(divine_comedy_split)\n",
        "dataset = dataset.window(seq_length + 1, step_length, drop_remainder=True)\n",
        "dataset = dataset.flat_map(lambda window: window.batch(seq_length + 1))\n",
        "dataset = dataset.map(split_input_target).shuffle(tot_samples, seed=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgKUP6qNEiKo",
        "colab_type": "text"
      },
      "source": [
        "Finally, we encode each block of the comedy using the provided `word_tokenizer` to tokenize the text into words, including punctuation\n",
        "\n",
        "> Some special tokens are reserved to the markers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bE2RTrqY36ZE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from text_processing.tokenizers import word_tokenizer\n",
        "\n",
        "tokenizer = word_tokenizer(divine_comedy)\n",
        "print(tokenizer.vocab_size, 'tokens:')\n",
        "print()\n",
        "for i, token in enumerate(tokenizer.tokens[:40]):\n",
        "  print(\"'{}'\".format('\\\\n' if token == '\\n' else token))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THUf94VHPnQT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_inp = 0\n",
        "max_tar = 0\n",
        "\n",
        "for inp, tar in dataset:\n",
        "  max_inp = max(max_inp, len(tokenizer.encode(inp.numpy())))\n",
        "  max_tar = max(max_tar, len(tokenizer.encode(tar.numpy())))\n",
        "\n",
        "# add two for the start/end tokens\n",
        "max_inp = max_inp + 2\n",
        "max_tar = max_tar + 2\n",
        "\n",
        "print('Max  Input:', max_inp)\n",
        "print('Max Target:', max_tar)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wqwv8CZKppn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def encode_dataset(input_dataset, target_dataset):\n",
        "  def encode_sample(inp, tar):\n",
        "    inp = [tokenizer.vocab_size] + tokenizer.encode(inp.numpy()) + [tokenizer.vocab_size+1]\n",
        "    tar = [tokenizer.vocab_size] + tokenizer.encode(tar.numpy()) + [tokenizer.vocab_size+1]\n",
        "\n",
        "    return pad_sequences([inp], maxlen=max_inp, padding='post')[0], pad_sequences([tar], maxlen=max_tar, padding='post')[0]\n",
        "  return tf.py_function(encode_sample, [input_dataset, target_dataset], [tf.int64, tf.int64])\n",
        "\n",
        "train_dataset = dataset.take(train_samples).map(encode_dataset)\n",
        "train_dataset = train_dataset.cache()\n",
        "train_dataset = train_dataset.batch(batch_size, drop_remainder=True)\n",
        "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "val_dataset = dataset.take(tot_samples - train_samples).map(encode_dataset)\n",
        "val_dataset = val_dataset.cache()\n",
        "val_dataset = val_dataset.batch(batch_size, drop_remainder=True)\n",
        "val_dataset = val_dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bZAWLNaJr93",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for input, target in train_dataset.take(1):\n",
        "  input = input.numpy()[0]\n",
        "  target = target.numpy()[0]\n",
        "\n",
        "  print(f'Input  Shape: {input.shape}')\n",
        "  print(f'Target Shape: {target.shape}')\n",
        "  print()\n",
        "\n",
        "  print('INPUT:\\n')\n",
        "  print(input)\n",
        "  print(tokenizer.decode([token for token in input if 0 < token < tokenizer.vocab_size]))\n",
        "  print('\\n\\n---------------------\\n\\n')\n",
        "  print('TARGET:\\n')\n",
        "  print(target)\n",
        "  print(tokenizer.decode([token for token in target if 0 < token < tokenizer.vocab_size]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLSm_SHYLPAd",
        "colab_type": "text"
      },
      "source": [
        "## **2. Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HQ5q_sIXMwh",
        "colab_type": "text"
      },
      "source": [
        "### ***2.1 Architecture***\n",
        "\n",
        "The model is the combination of two different models:\n",
        "\n",
        "* an *Encoder*, made up of an *Embedding* and a *GRU* layer, which takes the input string and returns both the output and the hidden state of the *GRU*\n",
        "\n",
        "* a *Decoder*, made up of an *Embedding* and a *GRU* layer as well as an *Attention* layer, which takes in input the latest decoded token (initially, this is the start token), the latest hidden state (initially, this is the hidden state of the encoder) and the output of the encoder, and returns the hidden state of the *GRU* and the output processed through a *Dense* layer\n",
        "\n",
        "> The variable parameters of the model are:\n",
        "> * the dimension of the *Embedding* layer\n",
        "> * the number of units of the *GRU* layer\n",
        "> * the kind of *Attention* layer, which can be either:\n",
        ">   - *Additive* (https://arxiv.org/pdf/1409.0473.pdf, Bahdanau et al.)\n",
        ">   -  *Multiplicative* (https://arxiv.org/pdf/1508.04025.pdf, Luong et al.)\n",
        "> * the dropout rate "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwwdMFK85XzP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.layers import Embedding, GRU, Reshape, Attention, AdditiveAttention, Concatenate, Dense\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "embedding_dim = 256\n",
        "gru_units = 1024\n",
        "attention = 'ADD'\n",
        "dropout = 0.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMgxWR0F745N",
        "colab_type": "text"
      },
      "source": [
        "#### *2.1.1 Encoder*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPZDGEeATZ7g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_input = Input((max_inp,), name='encoder_input')\n",
        "encoder_embedding = Embedding(tokenizer.vocab_size + 2, embedding_dim, name='encoder_embedding')(encoder_input)\n",
        "encoder_gru = GRU(\n",
        "    gru_units, return_sequences=True, return_state=True, stateful=False,\n",
        "    dropout=dropout, recurrent_initializer='glorot_uniform', name='encoder_gru'\n",
        ")(encoder_embedding)\n",
        "\n",
        "encoder = Model(encoder_input, encoder_gru, name='Encoder')\n",
        "\n",
        "display(plot_model(encoder, show_shapes=True, show_layer_names=False, rankdir='LR'))\n",
        "print()\n",
        "encoder.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMohQvv1772E",
        "colab_type": "text"
      },
      "source": [
        "#### *2.1.2 Decoder*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViSIDkxHTa_b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_input = Input((1,), name='decoder_input')\n",
        "hidden_state = Input((gru_units,), name='hidden_state')\n",
        "encoder_output = Input((max_inp, gru_units), name='encoder_output')\n",
        "expanded_hidden = Reshape((1, gru_units), name='expanded_hidden')(hidden_state)\n",
        "\n",
        "if attention == 'ADD':\n",
        "  decoder_attention = AdditiveAttention(name='decoder_attention')([expanded_hidden, encoder_output])\n",
        "elif attention == 'MUL':\n",
        "  decoder_attention = Attention(name='decoder_attention')([expanded_hidden, encoder_output])\n",
        "\n",
        "decoder_embedding = Embedding(tokenizer.vocab_size + 2, embedding_dim, name='decoder_embedding')(decoder_input)\n",
        "decoder_concatenate = Concatenate(name='decoder_concatenate')((decoder_attention, decoder_embedding))\n",
        "decoder_output, decoder_hidden = GRU(\n",
        "    gru_units, return_sequences=True, return_state=True, stateful=False,\n",
        "    dropout=dropout, recurrent_initializer='glorot_uniform', name='decoder_gru'\n",
        ")(decoder_concatenate)\n",
        "shrunk_output = Reshape((gru_units,), name='shrunk_output')(decoder_output)\n",
        "output = Dense(tokenizer.vocab_size + 2, name='output')(shrunk_output)\n",
        "\n",
        "decoder = Model([decoder_input, hidden_state, encoder_output], [output, decoder_hidden], name='Decoder')\n",
        "\n",
        "display(plot_model(decoder, show_shapes=True, show_layer_names=False))\n",
        "print()\n",
        "decoder.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sG39RjwxY-xB",
        "colab_type": "text"
      },
      "source": [
        "### ***2.2 Training***\n",
        "\n",
        "We need to write a custom training loop as, during the decoding phase, we will need to add one token at a time as well as setting the correct input state, then we will proceed with the training phase, storing every `epochs_interval` epochs the weights of the model in a file that indicates the values of its parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbKtSDN_9n51",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
        "\n",
        "history = {'train loss': [], 'train acc': [], 'val loss': [], 'val acc': []}\n",
        "\n",
        "optimizer = Adam()\n",
        "loss_object = SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "acc_object = SparseCategoricalAccuracy()\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  loss_ = loss_object(real, pred)\n",
        "  mask = tf.cast(tf.math.logical_not(tf.math.equal(real, 0)), dtype=loss_.dtype)\n",
        "  return tf.reduce_mean(loss_ * mask)\n",
        "\n",
        "@tf.function\n",
        "def train_step(inp, tar):\n",
        "  loss = 0\n",
        "  acc = 0\n",
        "  with tf.GradientTape() as tape:\n",
        "    # first of all, we callget the outputs from the encoder\n",
        "    encoder_output, hidden_state = encoder(inp)\n",
        "    \n",
        "    # the decoder input is initially made up of the start token only\n",
        "    decoder_input = tf.expand_dims([tokenizer.vocab_size] * batch_size, 1)\n",
        "\n",
        "    # for each token in the target\n",
        "    for t in range(1, tar.shape[1]):\n",
        "      # we get both the output and the hidden state of the decoder (this last one will be used in the next prediction)\n",
        "      predictions, hidden_state = decoder([decoder_input, hidden_state, encoder_output])\n",
        "\n",
        "      # then, we use the teacher forcing technique by feeding the target as the next input\n",
        "      decoder_input = tf.expand_dims(tar[:, t], 1)\n",
        "\n",
        "      # finally, we update the total loss and accuracy\n",
        "      loss += loss_function(tar[:, t], predictions)\n",
        "      acc += acc_object(tar[:, t], predictions)\n",
        "\n",
        "  # we apply the backpropagation both to the encoder and decoder variables\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  # and finally, we return the loss over the entire batch\n",
        "  return loss / int(tar.shape[1]), acc / int(tar.shape[1])\n",
        "\n",
        "@tf.function\n",
        "def val_step(inp, tar):\n",
        "  loss = 0\n",
        "  acc = 0\n",
        "  with tf.GradientTape() as tape:\n",
        "    encoder_output, hidden_state = encoder(inp)\n",
        "    decoder_input = tf.expand_dims([tokenizer.vocab_size] * batch_size, 1)\n",
        "    for t in range(1, tar.shape[1]):\n",
        "      predictions, hidden_state = decoder([decoder_input, hidden_state, encoder_output])\n",
        "      decoder_input = tf.expand_dims(tar[:, t], 1)\n",
        "      loss += loss_function(tar[:, t], predictions)\n",
        "      acc += acc_object(tar[:, t], predictions)\n",
        "  return loss / int(tar.shape[1]), acc / int(tar.shape[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyVQkUkHMXPI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "from utils.checkpoint import restore_checkpoint, save_checkpoint\n",
        "\n",
        "encoder_checkpoint_signature = 'Encoder seq_{} stp_{} btc_{} tvs_{} emb_{} unt_{} att_{} drp_{} epc_'.format(\n",
        "    seq_length, step_length, batch_size, train_val_split,\n",
        "    embedding_dim, gru_units, attention, dropout\n",
        ")\n",
        "decoder_checkpoint_signature = 'Decoder seq_{} stp_{} btc_{} tvs_{} emb_{} unt_{} att_{} drp_{} epc_'.format(\n",
        "    seq_length, step_length, batch_size, train_val_split,\n",
        "    embedding_dim, gru_units, attention, dropout\n",
        ")\n",
        "checkpoint_directory = 'ckpt/'\n",
        "initial_epoch = restore_checkpoint(encoder, checkpoint_directory, encoder_checkpoint_signature, verbose=True)\n",
        "assert initial_epoch == restore_checkpoint(decoder, checkpoint_directory, decoder_checkpoint_signature, verbose=False)\n",
        "\n",
        "epochs = 100\n",
        "epochs_interval = 10\n",
        "batches_interval = 20\n",
        "\n",
        "for epoch in range(initial_epoch, epochs):\n",
        "  start = time.time()\n",
        "  print(f'Starting Epoch {epoch+1}/{epochs}')\n",
        "\n",
        "  total_loss = 0\n",
        "  total_acc = 0\n",
        "  for (batch, (inp, tar)) in enumerate(train_dataset):\n",
        "    batch_loss, batch_acc = train_step(inp, tar)\n",
        "    total_loss += batch_loss\n",
        "    total_acc += batch_acc\n",
        "    if (batch + 1) % batches_interval == 0:\n",
        "      print(f'  > Batch {batch+1}', end=' \\t\\t ')\n",
        "      print(f'- train_loss: {batch_loss:.4f} - train_acc: {batch_acc:.4f}')\n",
        "\n",
        "  history['train loss'].append(total_loss / (batch + 1))\n",
        "  history['train acc'].append(total_acc / (batch + 1))\n",
        "\n",
        "  total_loss = 0\n",
        "  total_acc = 0\n",
        "  for (batch, (inp, tar)) in enumerate(val_dataset):\n",
        "    batch_loss, batch_acc = val_step(inp, tar)\n",
        "    total_loss += batch_loss\n",
        "    total_acc += batch_acc\n",
        "\n",
        "  history['val loss'].append(total_loss / (batch + 1))\n",
        "  history['val acc'].append(total_acc / (batch + 1))\n",
        "\n",
        "  elapsed = time.time() - start\n",
        "  print(f'Ending Epoch {epoch+1}/{epochs}', end=' \\t ')\n",
        "  print(f'- train_loss: {history[\"train loss\"][-1]:.4f} - train_acc: {history[\"train acc\"][-1]:.4f}', end=' ')\n",
        "  print(f'- val_loss: {history[\"val loss\"][-1]:.4f} - val_acc: {history[\"val acc\"][-1]:.4f}')\n",
        "  print(f'Elapsed Time {elapsed:.2f}s\\n')\n",
        "\n",
        "  save_checkpoint(encoder, epoch, checkpoint_directory, encoder_checkpoint_signature, epochs_interval, verbose=True)\n",
        "  save_checkpoint(decoder, epoch, checkpoint_directory, decoder_checkpoint_signature, epochs_interval, verbose=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nuvvp5hygYOv",
        "colab_type": "text"
      },
      "source": [
        "Here's a graphical representation of the improvement of the model, with respect both to the loss and the accuracy, across the epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GPN1Jo0gXpQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "if epochs - initial_epoch > 0:\n",
        "  sns.set_style('darkgrid')\n",
        "  sns.set_context('notebook')\n",
        "  plt.figure(figsize=(12, 5))\n",
        "\n",
        "  x = np.arange(initial_epoch, epochs) + 1\n",
        "\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.plot(x, history['train loss'], label='train')\n",
        "  plt.plot(x, history['val loss'], label='val')\n",
        "  plt.legend()\n",
        "  plt.title('Loss')\n",
        "\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.plot(x, history['train acc'], label='train')\n",
        "  plt.plot(x, history['val acc'], label='val')\n",
        "  plt.legend()\n",
        "  plt.title('Accuracy')\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGKbUSPZZOw_",
        "colab_type": "text"
      },
      "source": [
        "## **3. Generation**\n",
        "\n",
        "The generation is based on the trained model and it uses a `temperature_factor` to allow some degree of randomness\n",
        "\n",
        "> The next token is chosen among a subset of those having a probability which is at least `1 / temperature_factor` with respect to the maximal one\n",
        "\n",
        "> It goes without saying that a higher `temperature_factor` leads to a more explorative generation, while a lower `temperature_factor` leads to a more conservative one (in particular, with `temperature_factor = 1` the generation is completely deterministic)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyq4PC2hSL1Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.nn import softmax\n",
        "from text_processing.markers import unmark, MARKERS\n",
        "\n",
        "newline_token = tokenizer.encode('\\n')[0]\n",
        "\n",
        "def evaluate(inp_list, max_length=30, temperature_factor=1, verbose=False):\n",
        "  # the encoder input is the tokenized string obtained from the given input list surrounded by a start and an end token\n",
        "  encoder_input = [tokenizer.vocab_size] + tokenizer.encode('\\n'.join(inp_list)) + [tokenizer.vocab_size + 1]\n",
        "  encoder_input = pad_sequences([encoder_input], maxlen=max_inp, padding='post')\n",
        "\n",
        "  # initally, the decoder input is the start token \n",
        "  decoder_input = tf.expand_dims([tokenizer.vocab_size], 0)\n",
        "\n",
        "  # the final output of the evaluation (initially, this is an empty list)\n",
        "  output = []\n",
        "\n",
        "  # we repeat the same process of the training phase (teacher forcing) to reach the  get the correct hidden state up to the final input word\n",
        "  encoder_output, hidden_state = encoder(encoder_input)\n",
        "  for t in range(1, encoder_input.shape[1]):\n",
        "    predictions, hidden_state = decoder([decoder_input, hidden_state, encoder_output])\n",
        "    decoder_input = tf.expand_dims(encoder_input[:, t], 1)\n",
        "\n",
        "  # then we generate the remaining part\n",
        "  for t in range(max_length):\n",
        "    logits, hidden_state = decoder([decoder_input, hidden_state, encoder_output])\n",
        "\n",
        "    # we get the probabilities for the decoded token (special tokens excluded)\n",
        "    probabilities = softmax(logits[0, :tokenizer.vocab_size]).numpy()\n",
        "\n",
        "    # we take a subset of possible tokens whose probability is at least 1/temperature_factor of the maximal one\n",
        "    indices = np.arange(tokenizer.vocab_size)[probabilities >= probabilities.max() / temperature_factor]\n",
        "\n",
        "    # we renormalize this subset using, again, a softmax activation\n",
        "    probabilities = softmax(probabilities[probabilities >= probabilities.max() / temperature_factor]).numpy()\n",
        "    \n",
        "    # the id is randomly chosen among the indices according to the computed probabilities\n",
        "    predicted_id = np.random.choice(indices, size=1, p=probabilities)[0]\n",
        "    \n",
        "    # if the token coincides with the nd token or the newline token, the generation is interrupted\n",
        "    if predicted_id == newline_token or predicted_id >= tokenizer.vocab_size:\n",
        "      break\n",
        "\n",
        "    # totherwise the token is replaced as input and appended to the final output\n",
        "    decoder_input = tf.expand_dims([predicted_id], 0)\n",
        "    output.append(predicted_id)\n",
        "\n",
        "    if verbose:\n",
        "      print(tokenizer.decode([predicted_id]), end='')\n",
        "\n",
        "  return output\n",
        "\n",
        "def generate(\n",
        "    input_string=divine_comedy_marked[:386], # first three tercets of the comedy\n",
        "    max_iterations=250, end_marker=MARKERS['canto end'],\n",
        "    temperature_factor=1.0, verbose=False\n",
        "):\n",
        "  # at the beginning, the generated string is the encoding of the input string (plus a newline character)\n",
        "  generated_string = input_string\n",
        "\n",
        "  for i in range(max_iterations):\n",
        "    # the input list is made up of the last 'seq_length' verses (-1 for the last blank verse to be filled)\n",
        "    input_list = generated_string.split('\\n')[-seq_length-1:]\n",
        "\n",
        "    # the generated verse is then decoded\n",
        "    generated_verse = tokenizer.decode(evaluate(input_list, temperature_factor=temperature_factor, verbose=verbose))\n",
        "    if verbose:\n",
        "      print()\n",
        "\n",
        "    # if the verse coincides with the end marker, the generation is interrupted, otherwise it is appended with a newline\n",
        "    if generated_verse == end_marker:\n",
        "      break\n",
        "    generated_string += generated_verse + '\\n'\n",
        "  \n",
        "  # we finally return the decoded (and unmarked) string, excluding the input provided by the user\n",
        "  return unmark(generated_string[len(input_string):])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_93--aGpSLA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from metrics.store import store\n",
        "\n",
        "result_dir = results_path + model_name\n",
        "result_file = 'seq_{} stp_{} btc_{} tvs_{} emb_{} unt_{} att_{} drp_{}'.format(\n",
        "    seq_length, step_length, batch_size, train_val_split,\n",
        "    embedding_dim, gru_units, attention, dropout\n",
        ")\n",
        "\n",
        "print('Generating Temperature 1...')\n",
        "samples = [generate(temperature_factor=1)]\n",
        "store(result_dir, result_file, temperature=1, sample_texts=samples, original_text=divine_comedy)\n",
        "\n",
        "repetitions = 5\n",
        "for temperature in [2, 3, 5, 10]:\n",
        "  samples = []\n",
        "  for r in range(repetitions):\n",
        "    print(f'Generating Temperature {temperature} (repetition {r+1})...')\n",
        "    samples.append(generate(temperature_factor=temperature))\n",
        "  store(result_dir, result_file, temperature, sample_texts=samples, original_text=divine_comedy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qk15dWMUyklE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generated_canto = generate(temperature_factor=3.0, verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00cV1lfd5cph",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(generated_canto)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}