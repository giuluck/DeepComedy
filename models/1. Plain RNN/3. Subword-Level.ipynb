{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3. Subword-Level.ipynb","provenance":[{"file_id":"1bmIPg4r35mXZxxuK68Wj4NhzjWsHawUw","timestamp":1591279397164},{"file_id":"1-r-ETMpyG0nvvJ04rkVvvaLHoNq_lDAi","timestamp":1590830537926},{"file_id":"1I9uENHPUycWU26cmx227U53f6CCfWg2_","timestamp":1590591144637},{"file_id":"1BU8novDKFc9MSPSdfpTrCAB7E01PIbi7","timestamp":1590589899818},{"file_id":"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/text_generation.ipynb","timestamp":1590570606159}],"private_outputs":true,"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"n8bg-vup0iWu","colab_type":"text"},"source":["## **0. Preliminary Settings**"]},{"cell_type":"markdown","metadata":{"id":"DjtD5j6P5TzW","colab_type":"text"},"source":["First of all, we need to clone the repository to get access to the code and use utility functions inside the notebook"]},{"cell_type":"code","metadata":{"id":"In6Aynn15SAK","colab_type":"code","colab":{}},"source":["!git clone https://github.com/mazzio97/DeepComedy.git\n","\n","project_path = 'DeepComedy/'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SNTAI4EH1UvX","colab_type":"text"},"source":["This folder is then added to the system path so that the modules can be used inside the notebook"]},{"cell_type":"code","metadata":{"id":"qs_mxpvExrY7","colab_type":"code","colab":{}},"source":["import sys\n","\n","sys.path.append(project_path + 'src')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0hne6FCq1ZKZ","colab_type":"text"},"source":["Finally, the *Divine Comedy* is loaded and stored in a variable"]},{"cell_type":"code","metadata":{"id":"EOofZ_881Z2X","colab_type":"code","colab":{}},"source":["with open(project_path + 'res/divine_comedy.txt', 'r', encoding='ISO-8859-1') as f:\n","  divine_comedy = f.read()\n","\n","print(divine_comedy[:231])\n","print('\\n\\n[...]\\n\\n')\n","print(divine_comedy[-266:])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RIV2vg39xPYb","colab_type":"text"},"source":["Also, we set Python's, Numpy's/Keras' and Tensorflow's seeds to guarantee the maximal level of reproducibility\n","\n","> Though, the results could still differ a little bit due to other randomized routines called during the execution and the inner stochasticity introduced by parallel computing"]},{"cell_type":"code","metadata":{"id":"0kGuzr3KxQLY","colab_type":"code","colab":{}},"source":["import random\n","import numpy as np\n","import tensorflow as tf\n","\n","random.seed(0)\n","np.random.seed(0)\n","tf.random.set_seed(0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QsjTDW5425qP","colab_type":"text"},"source":["## **1. Data Processing**"]},{"cell_type":"markdown","metadata":{"id":"IsGmnOvt4FDs","colab_type":"text"},"source":["### ***1.1 Text Mark***\n","\n","We use the provided function `mark` to map the original *Divine Comedy* into a marked version containing:\n","\n","* a marker both at the beginning and at the end of each *cantica*\n","\n","* a marker both at the beginning and at the end of each *canto*\n","\n","* a marker between each couple of *tercets*"]},{"cell_type":"code","metadata":{"id":"-p6yoy-N17hj","colab_type":"code","colab":{}},"source":["from text_processing.markers import mark\n","\n","divine_comedy_marked = mark(divine_comedy)\n","print(divine_comedy_marked[:260])\n","print('\\n\\n[...]\\n\\n')\n","print(divine_comedy_marked[-319:])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_LYDG8rO5JdW","colab_type":"text"},"source":["### ***1.2 The Tokenizer***\n","\n","We use the provided `subword_tokenizer` to tokenize the text into subwords, including punctuation\n","\n","> Some special tokens are reserved to the markers"]},{"cell_type":"code","metadata":{"id":"bE2RTrqY36ZE","colab_type":"code","colab":{}},"source":["from text_processing.tokenizers import subword_tokenizer\n","\n","tokenizer = subword_tokenizer(divine_comedy, target_vocab_size=2048, max_subword_length=3)\n","print(tokenizer.vocab_size, 'tokens:')\n","print()\n","for i, token in enumerate(tokenizer.subwords[:40]):\n","  print(\"'{}'\".format('\\\\n' if token == '\\n' else token))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dM-Dk-OP-bZi","colab_type":"code","colab":{}},"source":["divine_comedy_tokenized = tokenizer.encode(divine_comedy_marked)\n","print(len(divine_comedy_tokenized))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tc-wXffP84Y2","colab_type":"code","colab":{}},"source":["tokenized_sample = divine_comedy_tokenized[:57]\n","print(tokenizer.decode(tokenized_sample))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iY8_AO929kCf","colab_type":"code","colab":{}},"source":["for token in tokenized_sample:\n","  print(token, '-->', tokenizer.decode([token]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TJEMLEjQ8sia","colab_type":"text"},"source":["### ***1.3 Building the Dataset***"]},{"cell_type":"markdown","metadata":{"id":"JN7gx3WVFqMn","colab_type":"text"},"source":["In order to understand which one should be the minimal length of a window sequence so that the net could be able to clearly have an insight about the thyming scheme, we compute which one is the maximal length of an encoded verse and take the minimal length as at least four verses "]},{"cell_type":"code","metadata":{"id":"HXpDR5zf8w16","colab_type":"code","colab":{}},"source":["# the newline token\n","newline = tokenizer.encode('\\n')[0]\n","\n","# the indices of each newline\n","indices = [i + 1 for i, t in enumerate(divine_comedy_tokenized) if t == newline]\n","\n","# the length of each verse (or marker)\n","verses_lengths = [end - start for start, end in zip([0] + indices, indices +  [len(divine_comedy_tokenized)])]\n","\n","# five verses (4 + tercet mark) should be enough to understand the rhyming scheme\n","sequences_lengths = [sum(verses_lengths[i:i+5]) for i in range(len(verses_lengths)-4)]\n","max(sequences_lengths)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WZZCK05QGHOL","colab_type":"text"},"source":["Given that the ***sequence length*** should be at least *109*, we set it as *128*, then we choose a ***step_length***, namely the value that indicates how often we decide to take a sample and, finally, a ***train/validation split*** percentage\n","\n","> Being the text very dense we cannot take a too small `step_length`, as it will lead both to a prohibitive training time and a lot of overfitting\n","\n","> In order to avoid this behaviour but having the most possible trustworthy set of data, we choose a medium `step_length` together with a small `train_val_split`, so that (at the cost of a quite more expensive training) we could easily monitor overfitting while still using a lot of training data"]},{"cell_type":"code","metadata":{"id":"p4M_wr57FK5j","colab_type":"code","colab":{}},"source":["seq_length = 128\n","step_length = 16\n","batch_size = 128\n","train_val_split = 0.3\n","\n","tot_samples = int((len(divine_comedy_tokenized) - seq_length) / step_length)\n","train_samples = round(tot_samples * train_val_split)\n","\n","print('Train Samples:', train_samples)\n","print('  Val Samples:', tot_samples - train_samples)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H8g_f4WqJ-Sq","colab_type":"text"},"source":["Finally, the tokenized dataset is split into windows of length `seq_length` (*+1*) sampled every `step_length` tokens and these windows are then shared into an *input sequence* and a *target sequence*, both of length `seq_length`, having an offset of one single token"]},{"cell_type":"code","metadata":{"id":"ePAQw8Pk75zq","colab_type":"code","colab":{}},"source":["from tensorflow.data import Dataset\n","\n","def split_input_target(chunk):\n","  input_sequence = chunk[:-1]\n","  target_sequence = chunk[1:]\n","  return input_sequence, target_sequence\n","\n","dataset = Dataset.from_tensor_slices(divine_comedy_tokenized)\n","dataset = dataset.window(seq_length + 1, step_length, drop_remainder=True)\n","dataset = dataset.flat_map(lambda window: window.batch(seq_length + 1))\n","dataset = dataset.map(split_input_target).shuffle(tot_samples, seed=0)\n","\n","train_dataset = dataset.take(train_samples).batch(batch_size)\n","val_dataset = dataset.take(tot_samples - train_samples).batch(batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3bZAWLNaJr93","colab_type":"code","colab":{}},"source":["for input, target in dataset.take(1):\n","  print('INPUT:\\n')\n","  print(tokenizer.decode(input))\n","  print('\\n\\n---------------------\\n\\n')\n","  print('TARGET:\\n')\n","  print(tokenizer.decode(target))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QLSm_SHYLPAd","colab_type":"text"},"source":["## **2. Model**"]},{"cell_type":"markdown","metadata":{"id":"1HQ5q_sIXMwh","colab_type":"text"},"source":["### ***2.1 Architecture***\n","\n","The model consists of an initial *Embedding* layer that maps the tokenized characters into a dense vector which is then passed to one or two *RNN* layer(s) and, eventually, to a final *Dense* layer, post-processed using *softmax* activation, which outputs the probability of each token\n","\n","> The variable parameters of the model are:\n","> * the dimension of the *Embedding* layer\n","> * the kind of *RNN* (*GRU* or *LSTM*)\n","> * the number of units of the *RNN* layers\n","> * the dropout rate "]},{"cell_type":"code","metadata":{"id":"W8njSEQOK48W","colab_type":"code","colab":{}},"source":["from tensorflow.keras import Model, Input\n","from tensorflow.keras.layers import Embedding, GRU, LSTM, Dense\n","from tensorflow.keras.utils import plot_model\n","\n","embedding_dim = 64\n","rnn_type = 'LSTM'\n","rnn_units_1 = 512\n","rnn_units_2 = None\n","dropout = 0.1\n","\n","def rnn_layer(units, name):\n","  if rnn_type == 'LSTM':\n","    return LSTM(\n","      units, dropout=dropout, return_sequences=True, stateful=False,\n","      recurrent_initializer='glorot_uniform', name=name\n","    )\n","  elif rnn_type == 'GRU':\n","    return GRU(\n","      units, dropout=dropout, return_sequences=True, stateful=False,\n","      recurrent_initializer='glorot_uniform', name=name\n","    )\n","\n","input_tensor = Input((seq_length,), dtype='int64', name='input')\n","embedding_tensor = Embedding(tokenizer.vocab_size, embedding_dim, name='embedding')(input_tensor)\n","rnn_tensor = rnn_layer(rnn_units_1, 'rnn_layer_1')(embedding_tensor)\n","if rnn_units_2 is not None:\n","    rnn_tensor = rnn_layer(rnn_units_2, 'rnn_layer_2')(rnn_tensor)\n","output_tensor = Dense(tokenizer.vocab_size, activation='softmax', name='output_layer')(rnn_tensor)\n","\n","model = Model(input_tensor, output_tensor, name='DeepComedy')\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","display(plot_model(model, show_shapes=True, show_layer_names=False, rankdir='LR'))\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sG39RjwxY-xB","colab_type":"text"},"source":["### ***2.2 Training***\n","\n","We can now proceed with the training phase, storing every `epochs_interval` epochs the weights of the model in a file that indicates the values of its parameters"]},{"cell_type":"code","metadata":{"id":"uyVQkUkHMXPI","colab_type":"code","colab":{}},"source":["from utils.validation import validation_callback\n","from utils.checkpoint import restore_checkpoint, checkpoint_callback\n","\n","checkpoint_signature = 'seq_{} stp_{} btc_{} tvs_{} emb_{} rnn_{} ru1_{} ru2_{} drp_{} epc_'.format(\n","    seq_length, step_length, batch_size, train_val_split,\n","    embedding_dim, rnn_type, rnn_units_1, rnn_units_2, dropout\n",")\n","checkpoint_directory = checkpoint_path + model_name\n","initial_epoch = restore_checkpoint(model, checkpoint_directory, checkpoint_signature)\n","\n","epochs = 50\n","epochs_interval = 10\n","batches_interval = 20\n","\n","val_callback, history = validation_callback(model, val_dataset, epochs, batches_interval)\n","ckp_callback = checkpoint_callback(model, checkpoint_directory, checkpoint_signature, epochs_interval)\n","model.fit(train_dataset, epochs=epochs, initial_epoch=initial_epoch, callbacks=[val_callback, ckp_callback], verbose=0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nuvvp5hygYOv","colab_type":"text"},"source":["Here's a graphical representation of the improvement of the model, with respect both to the loss and the accuracy, across the epochs"]},{"cell_type":"code","metadata":{"id":"6GPN1Jo0gXpQ","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","\n","if epochs - initial_epoch > 0:\n","  sns.set_style('darkgrid')\n","  sns.set_context('notebook')\n","  plt.figure(figsize=(12, 5))\n","\n","  x = np.arange(initial_epoch, epochs) + 1\n","\n","  plt.subplot(1, 2, 1)\n","  plt.plot(x, history['train loss'], label='train')\n","  plt.plot(x, history['val loss'], label='val')\n","  plt.legend()\n","  plt.title('Loss')\n","\n","  plt.subplot(1, 2, 2)\n","  plt.plot(x, history['train acc'], label='train')\n","  plt.plot(x, history['val acc'], label='val')\n","  plt.legend()\n","  plt.title('Accuracy')\n","\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TGKbUSPZZOw_","colab_type":"text"},"source":["## **3. Generation**\n","\n","The generation is based on the trained model and it uses a `temperature_factor` to allow some degree of randomness\n","\n","> The next token is chosen among a subset of those having a probability which is at least `1 / temperature_factor` with respect to the maximal one\n","\n","> It goes without saying that a higher `temperature_factor` leads to a more explorative generation, while a lower `temperature_factor` leads to a more conservative one (in particular, with `temperature_factor = 1` the generation is completely deterministic)"]},{"cell_type":"code","metadata":{"id":"ZqiZhhipjxF8","colab_type":"code","colab":{}},"source":["from tensorflow.nn import softmax\n","from text_processing.markers import unmark, MARKERS\n","\n","def generate(\n","    input_string=divine_comedy_marked[:386], # first three tercets of the comedy\n","    max_iterations=4000, end_marker=MARKERS['canto end'],\n","    temperature_factor=1.0, verbose=False\n","):\n","  # at the beginning, the generated string is the encoding of the input string\n","  generated_string = tokenizer.encode(input_string)\n","  \n","  for i in range(max_iterations):  \n","    # the input sequence is made up of the last 'seq_length' tokens of the generated string\n","    input_sequence = np.array([generated_string[-seq_length:]], dtype='int64')\n","    \n","    # we are interested in the probabilities for the last element of the sequence\n","    probabilities = model.predict(input_sequence)[0, -1]\n","\n","    # we take a subset of possible tokens whose probability is at least 1/temperature_factor of the maximal one\n","    indices = np.arange(tokenizer.vocab_size)[probabilities >= probabilities.max() / temperature_factor]\n","\n","    # we renormalize this subset using, again, a softmax activation\n","    probabilities = softmax(probabilities[probabilities >= probabilities.max() / temperature_factor]).numpy()\n","\n","    # the id is randomly chosen among the indices according to the computed probabilities\n","    predicted_id = np.random.choice(indices, size=1, p=probabilities)[0]\n","\n","    # the id is then mappend into a token from the vocabulary\n","    predicted_token = tokenizer.decode([predicted_id])\n","    if verbose:\n","      print(predicted_token, end='')\n","\n","    # if the token coincides with the end marker, the generation is interrupted, otherwise the token is appended \n","    if predicted_token == end_marker:\n","      break\n","    generated_string.append(predicted_id)\n","  \n","  # we finally return the decoded (and unmarked) string, excluding the input provided by the user\n","  return unmark(tokenizer.decode(generated_string)[len(input_string):])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qk15dWMUyklE","colab_type":"code","colab":{}},"source":["generated_canto = generate(temperature_factor=3.0, verbose=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dUXSajlKk2Hh","colab_type":"code","colab":{}},"source":["print(generated_canto)"],"execution_count":null,"outputs":[]}]}