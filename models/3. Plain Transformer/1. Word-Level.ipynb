{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "1. Word-Level.ipynb",
   "provenance": [
    {
     "file_id": "1bmIPg4r35mXZxxuK68Wj4NhzjWsHawUw",
     "timestamp": 1591279397164
    },
    {
     "file_id": "1-r-ETMpyG0nvvJ04rkVvvaLHoNq_lDAi",
     "timestamp": 1590830537926
    },
    {
     "file_id": "1I9uENHPUycWU26cmx227U53f6CCfWg2_",
     "timestamp": 1590591144637
    },
    {
     "file_id": "1BU8novDKFc9MSPSdfpTrCAB7E01PIbi7",
     "timestamp": 1590589899818
    },
    {
     "file_id": "https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/text_generation.ipynb",
     "timestamp": 1590570606159
    }
   ],
   "private_outputs": true,
   "collapsed_sections": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8bg-vup0iWu",
    "colab_type": "text"
   },
   "source": [
    "## **0. Preliminary Settings**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QRX8cEBb1brn",
    "colab_type": "text"
   },
   "source": [
    "Guarantee access to *Google Drive* to store partial results and checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "REof4kXgRY5_",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "checkpoint_path = '/content/drive/My Drive/Colab Notebooks/Models/'\n",
    "results_path = '/content/drive/My Drive/Colab Notebooks/Results/'\n",
    "model_name = '3. Plain Transformer/1. Word-Level/'"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DjtD5j6P5TzW",
    "colab_type": "text"
   },
   "source": [
    "First of all, we need to clone the repository to get access to the code and use utility functions inside the notebook"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "In6Aynn15SAK",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "!git clone https://github.com/mazzio97/DeepComedy.git\n",
    "\n",
    "project_path = 'DeepComedy/'"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SNTAI4EH1UvX",
    "colab_type": "text"
   },
   "source": [
    "This folder is then added to the system path so that the modules can be used inside the notebook"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qs_mxpvExrY7",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(project_path + 'src')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hne6FCq1ZKZ",
    "colab_type": "text"
   },
   "source": [
    "Finally, the *Divine Comedy* is loaded and stored in a variable"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EOofZ_881Z2X",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "with open(project_path + 'res/divine_comedy.txt', 'r', encoding='ISO-8859-1') as f:\n",
    "  divine_comedy = f.read()\n",
    "\n",
    "print(divine_comedy[:231])\n",
    "print('\\n\\n[...]\\n\\n')\n",
    "print(divine_comedy[-266:])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RIV2vg39xPYb",
    "colab_type": "text"
   },
   "source": [
    "Also, we set Python's, Numpy's/Keras' and Tensorflow's seeds to guarantee the maximal level of reproducibility\n",
    "\n",
    "> Though, the results could still differ a little bit due to other randomized routines called during the execution and the inner stochasticity introduced by parallel computing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0kGuzr3KxQLY",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QsjTDW5425qP",
    "colab_type": "text"
   },
   "source": [
    "## **1. Data Processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IsGmnOvt4FDs",
    "colab_type": "text"
   },
   "source": [
    "### ***1.1 Text Mark***\n",
    "\n",
    "We use the provided function `mark` to map the original *Divine Comedy* into a marked version containing:\n",
    "\n",
    "* a marker both at the beginning and at the end of each *cantica*\n",
    "\n",
    "* a marker both at the beginning and at the end of each *canto*\n",
    "\n",
    "* a marker between each couple of *tercets*"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-p6yoy-N17hj",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from text_processing.markers import mark\n",
    "\n",
    "divine_comedy_marked = mark(divine_comedy)\n",
    "print(divine_comedy_marked[:260])\n",
    "print('\\n\\n[...]\\n\\n')\n",
    "print(divine_comedy_marked[-319:])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_LYDG8rO5JdW",
    "colab_type": "text"
   },
   "source": [
    "### ***1.2 The Tokenizer***\n",
    "\n",
    "We use the provided `word_tokenizer` to tokenize the text into words, including punctuation\n",
    "\n",
    "> Some special tokens are reserved to the markers"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bE2RTrqY36ZE",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from text_processing.tokenizers import word_tokenizer\n",
    "\n",
    "tokenizer = word_tokenizer(divine_comedy)\n",
    "print(tokenizer.vocab_size, 'tokens:')\n",
    "print()\n",
    "for i, token in enumerate(tokenizer.tokens[:40]):\n",
    "  print(\"'{}'\".format('\\\\n' if token == '\\n' else token))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dM-Dk-OP-bZi",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "divine_comedy_tokenized = tokenizer.encode(divine_comedy_marked)\n",
    "print(len(divine_comedy_tokenized))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Tc-wXffP84Y2",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "tokenized_sample = divine_comedy_tokenized[:45]\n",
    "print(tokenizer.decode(tokenized_sample))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iY8_AO929kCf",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "for token in tokenized_sample:\n",
    "  print(token, '-->', tokenizer.decode([token]))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJEMLEjQ8sia",
    "colab_type": "text"
   },
   "source": [
    "### ***1.3 Building the Dataset***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JN7gx3WVFqMn",
    "colab_type": "text"
   },
   "source": [
    "In order to understand which one should be the minimal length of a window sequence so that the net could be able to clearly have an insight about the thyming scheme, we compute which one is the maximal length of an encoded verse and take the minimal length as at least four verses "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HXpDR5zf8w16",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# the newline token\n",
    "newline = tokenizer.encode('\\n')[0]\n",
    "\n",
    "# the indices of each newline\n",
    "indices = [i + 1 for i, t in enumerate(divine_comedy_tokenized) if t == newline]\n",
    "\n",
    "# the length of each verse (or marker)\n",
    "verses_lengths = [end - start for start, end in zip([0] + indices, indices +  [len(divine_comedy_tokenized)])]\n",
    "\n",
    "# five verses (4 + tercet mark) should be enough to understand the rhyming scheme\n",
    "sequences_lengths = [sum(verses_lengths[i:i+5]) for i in range(len(verses_lengths)-4)]\n",
    "max(sequences_lengths)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZZCK05QGHOL",
    "colab_type": "text"
   },
   "source": [
    "Given that the ***sequence length*** should be at least *95*, we set it as *126 (+2 special start/end tokens = 128)*, then we choose a ***step_length***, namely the value that indicates how often we decide to take a sample and, finally, a ***train/validation split*** percentage\n",
    "\n",
    "> Being the text very dense we cannot take a too small `step_length`, as it will lead both to a prohibitive training time and a lot of overfitting\n",
    "\n",
    "> In order to avoid this behaviour but having the most possible trustworthy set of data, we choose a medium `step_length` together with a small `train_val_split`, so that (at the cost of a quite more expensive training) we could easily monitor overfitting while still using a lot of training data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "p4M_wr57FK5j",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "seq_length = 126\n",
    "step_length = 8\n",
    "batch_size = 128\n",
    "train_val_split = 0.3\n",
    "\n",
    "tot_samples = int((len(divine_comedy_tokenized) - seq_length) / step_length)\n",
    "train_samples = round(tot_samples * train_val_split)\n",
    "\n",
    "print('Train Samples:', train_samples)\n",
    "print('  Val Samples:', tot_samples - train_samples)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H8g_f4WqJ-Sq",
    "colab_type": "text"
   },
   "source": [
    "Finally, the tokenized dataset is split into windows of length `seq_length` (*+1*) sampled every `step_length` tokens and these windows are then shared into an *input sequence* and a *target sequence*, both of length `seq_length`, having an offset of one single token"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ePAQw8Pk75zq",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from tensorflow.data import Dataset\n",
    "\n",
    "def split_input_target(chunk):\n",
    "  input_sequence = chunk[:-1]\n",
    "  target_sequence = chunk[1:]\n",
    "  return input_sequence, target_sequence\n",
    "\n",
    "dataset = Dataset.from_tensor_slices(divine_comedy_tokenized)\n",
    "dataset = dataset.window(seq_length + 1, step_length, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(seq_length + 1))\n",
    "dataset = dataset.map(split_input_target).shuffle(tot_samples, seed=0)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-wqwv8CZKppn",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def encode_dataset(input_dataset, target_dataset):\n",
    "  def encode_sample(input, target):\n",
    "    input = [tokenizer.vocab_size] + list(input.numpy()) + [tokenizer.vocab_size+1]\n",
    "    target = [tokenizer.vocab_size] + list(target.numpy()) + [tokenizer.vocab_size+1]\n",
    "    return input, target\n",
    "\n",
    "  input_dataset, target_dataset = tf.py_function(encode_sample, [input_dataset, target_dataset], [tf.int64, tf.int64])\n",
    "  input_dataset.set_shape([None])\n",
    "  target_dataset.set_shape([None])\n",
    "  return input_dataset, target_dataset\n",
    "\n",
    "train_dataset = dataset.take(train_samples).map(encode_dataset)\n",
    "train_dataset = train_dataset.cache()\n",
    "train_dataset = train_dataset.padded_batch(batch_size)\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "val_dataset = dataset.skip(train_samples).map(encode_dataset)\n",
    "val_dataset = val_dataset.cache()\n",
    "val_dataset = val_dataset.padded_batch(batch_size)\n",
    "val_dataset = val_dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3bZAWLNaJr93",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "for input, target in dataset.take(1):\n",
    "  print('INPUT:\\n')\n",
    "  print(tokenizer.decode(input))\n",
    "  print('\\n\\n---------------------\\n\\n')\n",
    "  print('TARGET:\\n')\n",
    "  print(tokenizer.decode(target))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLSm_SHYLPAd",
    "colab_type": "text"
   },
   "source": [
    "## **2. Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1HQ5q_sIXMwh",
    "colab_type": "text"
   },
   "source": [
    "### ***2.1 Architecture***\n",
    "\n",
    "The model consists of an initial *Embedding* layer that maps the tokenized characters into a dense vector which is then passed to one or two *RNN* layer(s) and, eventually, to a final *Dense* layer, post-processed using *softmax* activation, which outputs the probability of each token\n",
    "\n",
    "> The variable parameters of the model are:\n",
    "> * the dimension of the *Embedding* layer\n",
    "> * the kind of *RNN* (*GRU* or *LSTM*)\n",
    "> * the number of units of the *RNN* layers\n",
    "> * the forward and recurrent dropout rates "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lPZDGEeATZ7g",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from transformer.functions import *\n",
    "from transformer.model import *\n",
    "\n",
    "history = {'train loss': [], 'train acc': [], 'val loss': [], 'val acc': []}\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='val_accuracy')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)\n",
    "\n",
    "step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=step_signature)\n",
    "def train_step(inp, tar):\n",
    "  tar_inp = tar[:, :-1]\n",
    "  tar_real = tar[:, 1:]\n",
    "  \n",
    "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions, _ = model(inp, tar_inp, True, enc_padding_mask, combined_mask, dec_padding_mask)\n",
    "    loss = loss_function(tar_real, predictions)\n",
    "\n",
    "  gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "  \n",
    "  train_loss(loss)\n",
    "  train_accuracy(tar_real, predictions)\n",
    "\n",
    "@tf.function(input_signature=step_signature)\n",
    "def val_step(inp, tar):\n",
    "  tar_inp = tar[:, :-1]\n",
    "  tar_real = tar[:, 1:]\n",
    "  \n",
    "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions, _ = model(inp, tar_inp, True, enc_padding_mask, combined_mask, dec_padding_mask)\n",
    "    loss = loss_function(tar_real, predictions)\n",
    "\n",
    "  val_loss(loss)\n",
    "  val_accuracy(tar_real, predictions)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ViSIDkxHTa_b",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "num_layers = 6\n",
    "num_heads = 8\n",
    "d_model = 128\n",
    "dff = 512\n",
    "dropout = 0.1\n",
    "input_vocab_size = tokenizer.vocab_size + 2\n",
    "target_vocab_size = tokenizer.vocab_size + 2\n",
    "optimizer = Adam(CustomSchedule(d_model), beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "model = Transformer(\n",
    "    num_layers, d_model, num_heads, dff,\n",
    "    input_vocab_size, target_vocab_size,\n",
    "    pe_input=input_vocab_size, pe_target=target_vocab_size,\n",
    "    rate=dropout\n",
    ")\n",
    "\n",
    "for (inp, tar) in train_dataset.take(1):\n",
    "  val_step(inp, tar)\n",
    "\n",
    "model.summary()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sG39RjwxY-xB",
    "colab_type": "text"
   },
   "source": [
    "### ***2.2 Training***\n",
    "\n",
    "We can now proceed with the training phase, storing every `epochs_interval` epochs the weights of the model in a file that indicates the values of its parameters"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uyVQkUkHMXPI",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import time\n",
    "from utils.checkpoint import restore_checkpoint, save_checkpoint\n",
    "\n",
    "checkpoint_signature = 'seq_{} stp_{} btc_{} tvs_{} nlr_{} nhd_{} dmd_{} dff_{} drp_{} epc_'.format(\n",
    "    seq_length, step_length, batch_size, train_val_split,\n",
    "    num_layers, num_heads, d_model, dff, dropout\n",
    ")\n",
    "checkpoint_directory = checkpoint_path + model_name\n",
    "initial_epoch = restore_checkpoint(model, checkpoint_directory, checkpoint_signature)\n",
    "\n",
    "epochs = 50\n",
    "epochs_interval = 10\n",
    "batches_interval = 20\n",
    "\n",
    "for epoch in range(initial_epoch, epochs):\n",
    "  start = time.time()\n",
    "  train_loss.reset_states()\n",
    "  train_accuracy.reset_states()\n",
    "  val_loss.reset_states()\n",
    "  val_accuracy.reset_states()\n",
    "  print(f'Starting Epoch {epoch+1}/{epochs}')\n",
    "  \n",
    "  for (batch, (inp, tar)) in enumerate(train_dataset):\n",
    "    train_step(inp, tar)\n",
    "    if (batch + 1) % batches_interval == 0:\n",
    "      print(f'  > Batch {batch+1}', end=' \\t\\t ')\n",
    "      print(f'- train_loss: {train_loss.result():.4f} - train_acc: {train_accuracy.result():.4f}')      \n",
    "  history['train loss'].append(train_loss.result())\n",
    "  history['train acc'].append(train_accuracy.result())\n",
    "\n",
    "  for (batch, (inp, tar)) in enumerate(val_dataset):\n",
    "    val_step(inp, tar)  \n",
    "  history['val loss'].append(val_loss.result())\n",
    "  history['val acc'].append(val_accuracy.result())\n",
    "\n",
    "  elapsed = time.time() - start\n",
    "  print(f'Ending Epoch {epoch+1}/{epochs}', end=' \\t ')\n",
    "  print(f'- train_loss: {history[\"train loss\"][-1]:.4f} - train_acc: {history[\"train acc\"][-1]:.4f}', end=' ')\n",
    "  print(f'- val_loss: {history[\"val loss\"][-1]:.4f} - val_acc: {history[\"val acc\"][-1]:.4f}')\n",
    "  print(f'Elapsed Time {elapsed:.2f}s\\n')\n",
    "\n",
    "  save_checkpoint(model, epoch, checkpoint_directory, checkpoint_signature, epochs_interval, verbose=True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nuvvp5hygYOv",
    "colab_type": "text"
   },
   "source": [
    "Here's a graphical representation of the improvement of the model, with respect both to the loss and the accuracy, across the epochs"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6GPN1Jo0gXpQ",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "if epochs - initial_epoch > 0:\n",
    "  sns.set_style('darkgrid')\n",
    "  sns.set_context('notebook')\n",
    "  plt.figure(figsize=(12, 5))\n",
    "\n",
    "  x = np.arange(initial_epoch, epochs) + 1\n",
    "\n",
    "  plt.subplot(1, 2, 1)\n",
    "  plt.plot(x, history['train loss'], label='train')\n",
    "  plt.plot(x, history['val loss'], label='val')\n",
    "  plt.legend()\n",
    "  plt.title('Loss')\n",
    "\n",
    "  plt.subplot(1, 2, 2)\n",
    "  plt.plot(x, history['train acc'], label='train')\n",
    "  plt.plot(x, history['val acc'], label='val')\n",
    "  plt.legend()\n",
    "  plt.title('Accuracy')\n",
    "\n",
    "  plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGKbUSPZZOw_",
    "colab_type": "text"
   },
   "source": [
    "## **3. Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05Mk_Fqnjj2I",
    "colab_type": "text"
   },
   "source": [
    "### ***3.1 Generation***\n",
    "\n",
    "The generation is based on the trained model and it uses a `temperature_factor` to allow some degree of randomness\n",
    "\n",
    "> The next token is chosen among a subset of those having a probability which is at least `1 / temperature_factor` with respect to the maximal one\n",
    "\n",
    "> It goes without saying that a higher `temperature_factor` leads to a more explorative generation, while a lower `temperature_factor` leads to a more conservative one (in particular, with `temperature_factor = 1` the generation is completely deterministic)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "z2zd-C3Hle31",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from tensorflow.nn import softmax\n",
    "from text_processing.markers import unmark, MARKERS\n",
    "\n",
    "def evaluate(inp_sentence):\n",
    "  # the encoder input is the input sentence surrounded by a start and an end token\n",
    "  encoder_input = tf.expand_dims([tokenizer.vocab_size] + inp_sentence + [tokenizer.vocab_size + 1], 0)\n",
    "\n",
    "  # the decoder input is the same sentence, without the first character, preceded by a start token\n",
    "  decoder_input = tf.expand_dims([tokenizer.vocab_size] + inp_sentence[1:], 0)  \n",
    "\n",
    "  # we now call the model to get the logits tensor\n",
    "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, decoder_input)\n",
    "  logits, attention_weights = model(\n",
    "      encoder_input, decoder_input, False,\n",
    "      enc_padding_mask, combined_mask, dec_padding_mask\n",
    "  )\n",
    "\n",
    "  # finally, we return the logits for the decoded character\n",
    "  return logits[0, len(inp_sentence)-1, :].numpy()[:tokenizer.vocab_size]\n",
    "\n",
    "def generate(\n",
    "    input_string=divine_comedy_marked[:386], # first three tercets of the comedy\n",
    "    max_iterations=3000, end_marker=MARKERS['canto end'],\n",
    "    temperature_factor=1.0, verbose=False\n",
    "):\n",
    "  # at the beginning, the generated string is the encoding of the input string\n",
    "  generated_string = tokenizer.encode(input_string)\n",
    "\n",
    "  for i in range(max_iterations):\n",
    "    # the input sequence is made up of the last 'seq_length' tokens of the generated string\n",
    "    input_sequence = generated_string[-seq_length:]\n",
    "\n",
    "    # as the evaluate function returns logits, we need to apply a softmax activation to get probabilities\n",
    "    probabilities = softmax(evaluate(input_sequence)).numpy()\n",
    "\n",
    "    # we take a subset of possible tokens whose probability is at least 1/temperature_factor of the maximal one\n",
    "    indices = np.arange(tokenizer.vocab_size)[probabilities >= probabilities.max() / temperature_factor]\n",
    "\n",
    "    # we renormalize this subset using, again, a softmax activation\n",
    "    probabilities = softmax(probabilities[probabilities >= probabilities.max() / temperature_factor]).numpy()\n",
    "\n",
    "    # the id is randomly chosen among the indices according to the computed probabilities\n",
    "    predicted_id = np.random.choice(indices, size=1, p=probabilities)[0]\n",
    "\n",
    "    # the id is then mappend into a token from the vocabulary\n",
    "    predicted_token = tokenizer.decode([predicted_id])\n",
    "    if verbose:\n",
    "      print(predicted_token, end='')\n",
    "\n",
    "    # if the token coincides with the end marker, the generation is interrupted, otherwise the token is appended \n",
    "    if predicted_token == end_marker:\n",
    "      break\n",
    "    generated_string.append(predicted_id)\n",
    "  \n",
    "  # we finally return the decoded (and unmarked) string, excluding the input provided by the user\n",
    "  return unmark(tokenizer.decode(generated_string)[len(input_string):])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dgXV_DFtpYDl",
    "colab_type": "text"
   },
   "source": [
    "### ***3.2 Results***\n",
    "\n",
    "We then try different kind of temperatures and take an average evaluation, then, finally, we use the `store` utility function to store the results in a *.txt* file"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "R_93--aGpSLA",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import os\n",
    "from metrics.store import store\n",
    "\n",
    "result_dir = results_path + model_name\n",
    "result_file = 'seq_{} stp_{} btc_{} tvs_{} nlr_{} nhd_{} dmd_{} dff_{} drp_{}.txt'.format(\n",
    "    seq_length, step_length, batch_size, train_val_split,\n",
    "    num_layers, num_heads, d_model, dff, dropout\n",
    ")\n",
    "\n",
    "print('Generating Temperature 1...')\n",
    "samples = [generate(temperature_factor=1)]\n",
    "store(result_dir, result_file, temperature=1, sample_texts=samples, original_text=divine_comedy)\n",
    "\n",
    "repetitions = 5\n",
    "for temperature in [2, 3, 5]:\n",
    "  samples = []\n",
    "  for r in range(repetitions):\n",
    "    print(f'Generating Temperature {temperature} (repetition {r+1})...')\n",
    "    samples.append(generate(temperature_factor=temperature))\n",
    "  store(result_dir, result_file, temperature, sample_texts=samples, original_text=divine_comedy)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qk15dWMUyklE",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}